#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ë„êµ¬
- stock_moonrabbit ë¸”ë¡œê·¸ ìµœì‹  í¬ìŠ¤íŒ… ìˆ˜ì§‘
- ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
- ì¢…ëª© ë¶„ì„ í‚¤ì›Œë“œ ì¶”ì¶œ
"""

import asyncio
import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from playwright.async_api import async_playwright

BLOG_ID = "stock_moonrabbit"
BASE_URL = f"https://m.blog.naver.com/{BLOG_ID}"
OUTPUT_DIR = Path(__file__).parent / "blog_posts"


async def get_post_list(page, limit=5):
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    # PostList í˜ì´ì§€ë¡œ ì´ë™ (ì „ì²´ê¸€ë³´ê¸°)
    list_url = f"https://m.blog.naver.com/PostList.naver?blogId={BLOG_ID}&categoryNo=0"
    await page.goto(list_url)
    await page.wait_for_timeout(2000)

    # ìŠ¤í¬ë¡¤í•´ì„œ ë” ë§ì€ í¬ìŠ¤íŠ¸ ë¡œë“œ
    for _ in range(3):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(1000)

    posts = []
    seen_ids = set()

    # logNoê°€ ìˆëŠ” í¬ìŠ¤íŠ¸ ë§í¬ ì°¾ê¸°
    post_links = await page.query_selector_all('a[href*="logNo"]')
    print(f"  ë°œê²¬ëœ í¬ìŠ¤íŠ¸ ë§í¬: {len(post_links)}ê°œ")

    for link in post_links:
        try:
            href = await link.get_attribute("href")
            if not href or "logNo=" not in href:
                continue

            # í¬ìŠ¤íŠ¸ ID ì¶”ì¶œ
            post_id = href.split("logNo=")[-1].split("&")[0]
            if post_id in seen_ids:
                continue
            seen_ids.add(post_id)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª©)
            text = await link.inner_text()
            text = text.replace('\n', ' ').strip()

            # "ì‚¬ì§„ ê°œìˆ˜ X" ê°™ì€ ê±´ ì‹¤ì œ ì œëª©ì´ ì•„ë‹˜ - ë‚˜ì¤‘ì— í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜´
            if text.startswith("ì‚¬ì§„ ê°œìˆ˜") or len(text) < 5:
                title = f"(í¬ìŠ¤íŠ¸ {post_id})"
            else:
                title = text[:100]

            # URL ì •ê·œí™”
            if not href.startswith("http"):
                href = f"https://m.blog.naver.com{href}"

            posts.append({
                "id": post_id,
                "title": title,
                "date": "",
                "url": href
            })

            if len(posts) >= limit:
                break

        except Exception as e:
            print(f"  í¬ìŠ¤íŠ¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
            continue

    return posts


async def get_post_content(page, post_url):
    """ê°œë³„ í¬ìŠ¤íŠ¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° - ì œëª©, ë‚ ì§œ, ë³¸ë¬¸ ë°˜í™˜"""
    await page.goto(post_url)
    await page.wait_for_timeout(2000)

    result = {"title": "", "date": "", "content": ""}

    # ì œëª© ì¶”ì¶œ
    for sel in ["h3.se_title", "[class*='title']", "h3", ".tit_h3"]:
        title_elem = await page.query_selector(sel)
        if title_elem:
            result["title"] = (await title_elem.inner_text()).strip()
            if result["title"]:
                break

    # ë‚ ì§œ ì¶”ì¶œ
    for sel in ["[class*='date']", "time", ".blog_date", "span.se_publishDate"]:
        date_elem = await page.query_selector(sel)
        if date_elem:
            date_text = await date_elem.inner_text()
            if date_text and any(c.isdigit() for c in date_text):
                result["date"] = date_text.strip()
                break

    # ë³¸ë¬¸ ì¶”ì¶œ
    for sel in ["div.se-main-container", "div#postViewArea", "div.post-view",
                "[class*='post_content']", "[class*='se_component_wrap']"]:
        content_elem = await page.query_selector(sel)
        if content_elem:
            result["content"] = (await content_elem.inner_text()).strip()
            if len(result["content"]) > 50:
                break

    return result


def extract_tickers(text):
    """í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì‹¬ë³¼ ì¶”ì¶œ"""
    # ëŒ€ë¬¸ì 2-5ê¸€ì íŒ¨í„´ (ì¼ë°˜ì ì¸ í‹°ì»¤)
    tickers = re.findall(r'\b([A-Z]{2,5})\b', text)

    # í•œê¸€ë¡œ ëœ ì¢…ëª©ëª…ë„ ì°¾ê¸°
    korean_stocks = re.findall(r'([ê°€-í£]+)\s*\(([A-Z]{2,5})\)', text)

    # ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
    common_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL',
                   'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'OUT', 'ETF', 'CEO',
                   'IPO', 'FDA', 'SEC', 'NYSE', 'NASDAQ', 'USD', 'KRW'}

    filtered = [t for t in tickers if t not in common_words]

    return {
        "tickers": list(set(filtered)),
        "korean_names": korean_stocks
    }


def extract_keywords(text):
    """íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    keywords = {
        "ìˆìŠ¤í€´ì¦ˆ": ["ìˆìŠ¤í€´ì¦ˆ", "short squeeze", "ìˆì»¤ë²„", "ê³µë§¤ë„"],
        "ê¸‰ë“±": ["ê¸‰ë“±", "í­ë“±", "ìƒìŠ¹", "ìƒí•œê°€"],
        "ê¸‰ë½": ["ê¸‰ë½", "í­ë½", "í•˜ë½", "í•˜í•œê°€"],
        "ì‹¤ì ": ["ì‹¤ì ", "ì–´ë‹", "earnings", "ë§¤ì¶œ"],
        "FDA": ["FDA", "ìŠ¹ì¸", "ì„ìƒ", "ì‹ ì•½"],
        "ì¸ìˆ˜í•©ë³‘": ["ì¸ìˆ˜", "í•©ë³‘", "M&A", "merger"],
        "ë°°ë‹¹": ["ë°°ë‹¹", "dividend"],
        "í…Œë§ˆ": ["í…Œë§ˆ", "ì„¹í„°", "AI", "ë°˜ë„ì²´", "2ì°¨ì „ì§€", "ë°”ì´ì˜¤"]
    }

    found = []
    text_lower = text.lower()

    for category, terms in keywords.items():
        for term in terms:
            if term.lower() in text_lower:
                found.append(category)
                break

    return list(set(found))


async def scrape_blog(days_back=2, limit=10):
    """ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ë©”ì¸ í•¨ìˆ˜"""
    OUTPUT_DIR.mkdir(exist_ok=True)

    cutoff_date = datetime.now() - timedelta(days=days_back)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        print(f"ğŸ“¡ {BLOG_ID} ë¸”ë¡œê·¸ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

        # í¬ìŠ¤íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        posts = await get_post_list(page, limit=limit)
        print(f"ğŸ“ {len(posts)}ê°œ í¬ìŠ¤íŠ¸ ë°œê²¬")

        results = []

        for post in posts:
            print(f"  â†’ {post['title'][:30]}...")

            # ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ì œëª©, ë‚ ì§œ, ë³¸ë¬¸)
            post_detail = await get_post_content(page, post['url'])

            # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¨ ì œëª© ì‚¬ìš©
            if post['title'].startswith("(í¬ìŠ¤íŠ¸") and post_detail['title']:
                post['title'] = post_detail['title']
            if post_detail['date']:
                post['date'] = post_detail['date']

            content = post_detail['content']

            # ë¶„ì„
            tickers = extract_tickers(content)
            keywords = extract_keywords(content)

            post_data = {
                **post,
                "content": content,
                "tickers": tickers,
                "keywords": keywords,
                "scraped_at": datetime.now().isoformat()
            }

            results.append(post_data)

            # ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì €ì¥
            safe_title = re.sub(r'[^\w\s-]', '', post['title'])[:50]
            filename = f"{post.get('date', 'unknown')[:10]}_{safe_title}.md"
            filepath = OUTPUT_DIR / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"# {post['title']}\n\n")
                f.write(f"- **ë‚ ì§œ**: {post.get('date', 'N/A')}\n")
                f.write(f"- **URL**: {post['url']}\n")
                f.write(f"- **í‹°ì»¤**: {', '.join(tickers['tickers']) or 'N/A'}\n")
                f.write(f"- **í‚¤ì›Œë“œ**: {', '.join(keywords) or 'N/A'}\n\n")
                f.write("---\n\n")
                f.write(content)

            print(f"    âœ… ì €ì¥: {filename}")

        await browser.close()

        # ìš”ì•½ JSON ì €ì¥
        summary_path = OUTPUT_DIR / "latest_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ì™„ë£Œ! {len(results)}ê°œ í¬ìŠ¤íŠ¸ ì €ì¥ë¨")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")

        return results


def print_summary(results):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*50)
    print("ğŸ“Š ë¸”ë¡œê·¸ ë¶„ì„ ìš”ì•½")
    print("="*50)

    all_tickers = set()
    all_keywords = set()

    for post in results:
        print(f"\n### {post['title'][:40]}...")
        print(f"    í‹°ì»¤: {', '.join(post['tickers']['tickers']) or '-'}")
        print(f"    í‚¤ì›Œë“œ: {', '.join(post['keywords']) or '-'}")

        all_tickers.update(post['tickers']['tickers'])
        all_keywords.update(post['keywords'])

    print("\n" + "-"*50)
    print(f"ğŸ¯ ì „ì²´ ì–¸ê¸‰ í‹°ì»¤: {', '.join(sorted(all_tickers)) or 'N/A'}")
    print(f"ğŸ·ï¸ ì „ì²´ í‚¤ì›Œë“œ: {', '.join(sorted(all_keywords)) or 'N/A'}")

    return {
        "all_tickers": list(all_tickers),
        "all_keywords": list(all_keywords)
    }


if __name__ == "__main__":
    results = asyncio.run(scrape_blog(days_back=2, limit=5))
    summary = print_summary(results)
