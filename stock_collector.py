#!/usr/bin/env python3
"""
Data Collector v3 (Lite)
- ì£¼ê°€ ìˆ˜ì§‘ ì œê±° (yfinance ì‹¤ì‹œê°„ ì‚¬ìš©)
- ìˆìŠ¤í€´ì¦ˆ ìˆ˜ì§‘ ì œê±° (deep_analyzerê°€ ì‹¤ì‹œê°„ ë¶„ì„)
- RegSHO, í™˜ìœ¨, ë¸”ë¡œê·¸ë§Œ ìˆ˜ì§‘
"""

import asyncio
import json
import re
import os
from datetime import datetime

import psycopg2
from psycopg2.extras import RealDictCursor
from playwright.async_api import async_playwright

# ============================================================
# ì„¤ì •
# ============================================================

DB_URL = os.getenv("DATABASE_URL", "postgresql://claude:claude_dev@localhost:5432/continuous_claude")
BLOG_ID = "stock_moonrabbit"

# í¬íŠ¸í´ë¦¬ì˜¤ (RegSHO ì²´í¬ìš©)
HOLDINGS = [
    {"ticker": "BNAI", "shares": 464, "avg_cost": 9.55},
    {"ticker": "GLSI", "shares": 67, "avg_cost": 25.22},
]

# ì›Œì¹˜ë¦¬ìŠ¤íŠ¸
WATCHLIST = [
    {"ticker": "HIMS", "reason": "Short 31%, GLP-1 í…Œë§ˆ"},
    {"ticker": "SOUN", "reason": "Short 28%, AI í…Œë§ˆ"},
]


# ============================================================
# DB í•¨ìˆ˜
# ============================================================

def get_db():
    return psycopg2.connect(DB_URL)


def init_db():
    """í…Œì´ë¸” ìƒì„±"""
    conn = get_db()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS regSHO_list (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            security_name TEXT,
            market_category VARCHAR(10),
            first_seen_date DATE DEFAULT CURRENT_DATE,
            collected_date DATE DEFAULT CURRENT_DATE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blog_posts (
            id SERIAL PRIMARY KEY,
            post_id VARCHAR(30) UNIQUE,
            title TEXT,
            content TEXT,
            tickers TEXT[],
            keywords TEXT[],
            post_date VARCHAR(50),
            url TEXT,
            is_new BOOLEAN DEFAULT TRUE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS exchange_rates (
            id SERIAL PRIMARY KEY,
            from_currency VARCHAR(3),
            to_currency VARCHAR(3),
            rate DECIMAL(10,4),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blogger_tickers (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            mentioned_in_post VARCHAR(30),
            ticker_info JSONB,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE INDEX IF NOT EXISTS idx_blog_posts_new ON blog_posts(is_new, collected_at DESC);
    """)

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ")


def get_existing_post_ids():
    """ì´ë¯¸ ì €ì¥ëœ í¬ìŠ¤íŠ¸ ID ëª©ë¡"""
    conn = get_db()
    cur = conn.cursor()
    cur.execute("SELECT post_id FROM blog_posts")
    ids = {row[0] for row in cur.fetchall()}
    cur.close()
    conn.close()
    return ids


# ============================================================
# RegSHO ìˆ˜ì§‘
# ============================================================

async def collect_regSHO(page):
    """RegSHO Threshold List ìˆ˜ì§‘"""
    url = "https://www.nasdaqtrader.com/trader.aspx?id=regshothreshold"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    content = await page.content()

    tickers = []
    rows = re.findall(r'>([A-Z]{2,5})</td>\s*<td[^>]*>([^<]+)</td>\s*<td[^>]*>([QGS])</td>', content)

    for ticker, name, market in rows:
        tickers.append({
            "ticker": ticker,
            "name": name.strip(),
            "market": market
        })

    print(f"  RegSHO: {len(tickers)}ê°œ ì¢…ëª©")
    return tickers


# ============================================================
# í™˜ìœ¨ ìˆ˜ì§‘
# ============================================================

async def collect_exchange_rate(page):
    """USD/KRW í™˜ìœ¨ ìˆ˜ì§‘"""
    url = "https://www.google.com/finance/quote/USD-KRW"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    text = await page.inner_text("body")

    match = re.search(r'(\d{1,3}(?:,\d{3})*\.?\d*)\s*(?:KRW|ì›)', text)
    if match:
        rate = float(match.group(1).replace(',', ''))
    else:
        rate = 1450.0

    print(f"  í™˜ìœ¨: $1 = â‚©{rate:,.2f}")
    return rate


# ============================================================
# ë¸”ë¡œê·¸ ìˆ˜ì§‘
# ============================================================

async def collect_blog_posts(page, limit=10):
    """ìƒˆ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘"""
    existing_ids = get_existing_post_ids()
    print(f"  ê¸°ì¡´ í¬ìŠ¤íŠ¸: {len(existing_ids)}ê°œ")

    list_url = f"https://m.blog.naver.com/PostList.naver?blogId={BLOG_ID}&categoryNo=0"
    await page.goto(list_url)
    await page.wait_for_timeout(2000)

    for _ in range(5):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(500)

    links = await page.query_selector_all('a[href*="logNo"]')

    new_posts = []
    seen = set()

    for link in links:
        try:
            href = await link.get_attribute("href")
            if not href or "logNo=" not in href:
                continue

            post_id = href.split("logNo=")[-1].split("&")[0]

            if post_id in seen or post_id in existing_ids:
                continue
            seen.add(post_id)

            if not href.startswith("http"):
                href = f"https://m.blog.naver.com{href}"

            new_posts.append({"post_id": post_id, "url": href})

            if len(new_posts) >= limit:
                break

        except Exception:
            continue

    print(f"  ìƒˆ í¬ìŠ¤íŠ¸: {len(new_posts)}ê°œ ë°œê²¬")

    for post in new_posts:
        try:
            await page.goto(post["url"])
            await page.wait_for_timeout(2000)

            post["title"] = ""
            for sel in ["h3.se_title", "[class*='title']", "h3"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["title"] = (await elem.inner_text()).strip()[:200]
                    if post["title"]:
                        break

            post["date"] = ""
            for sel in ["[class*='date']", "time"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["date"] = (await elem.inner_text()).strip()
                    break

            post["content"] = ""
            for sel in ["div.se-main-container", "div#postViewArea"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["content"] = (await elem.inner_text()).strip()[:10000]
                    break

            content = post.get("content", "")
            raw_tickers = set(re.findall(r'\b([A-Z]{2,5})\b', content))
            common = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'SEC', 'ETF', 'CEO', 'IPO', 'FDA', 'NYSE', 'USD', 'KRW', 'SPAC', 'PIPE'}
            post["tickers"] = list(raw_tickers - common)

            keywords = []
            kw_map = {
                "ìˆìŠ¤í€´ì¦ˆ": ["ìˆìŠ¤í€´ì¦ˆ", "short squeeze", "ìˆì»¤ë²„"],
                "ê°•ì œì²­ì‚°": ["ê°•ì œì²­ì‚°", "forced buy", "close out"],
                "RegSHO": ["regsho", "reg sho", "threshold"],
                "FTD": ["ftd", "fail to deliver"],
                "ê¸‰ë“±": ["ê¸‰ë“±", "í­ë“±", "ìƒìŠ¹"],
                "ê¸‰ë½": ["ê¸‰ë½", "í­ë½", "í•˜ë½"],
            }
            content_lower = content.lower()
            for kw, patterns in kw_map.items():
                if any(p.lower() in content_lower for p in patterns):
                    keywords.append(kw)
            post["keywords"] = keywords

            print(f"    â†’ {post['title'][:40]}... | í‹°ì»¤: {post['tickers'][:5]}")

        except Exception as e:
            print(f"    â†’ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return new_posts


async def collect_blogger_ticker_info(page, tickers):
    """ë¸”ë¡œê±°ê°€ ì–¸ê¸‰í•œ í‹°ì»¤ë“¤ì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (yfinance)"""
    import yfinance as yf

    ticker_info = {}

    for ticker in tickers[:10]:
        try:
            stock = yf.Ticker(ticker)
            info = stock.info

            ticker_info[ticker] = {
                "name": info.get("shortName") or info.get("longName"),
                "sector": info.get("sector"),
                "industry": info.get("industry"),
                "market_cap": info.get("marketCap"),
                "price": info.get("regularMarketPrice"),
            }
            print(f"    {ticker}: {ticker_info[ticker].get('name', 'N/A')}")

        except Exception as e:
            print(f"    {ticker}: ì‹¤íŒ¨ - {e}")
            ticker_info[ticker] = {"name": None, "error": str(e)}

    return ticker_info


# ============================================================
# DB ì €ì¥
# ============================================================

def save_to_db(regSHO, exchange_rate, blog_posts, blogger_tickers):
    """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    conn = get_db()
    cur = conn.cursor()

    # RegSHO ì €ì¥ (ì—°ì† ë“±ì¬ì¼ ì¶”ì )
    cur.execute("""
        SELECT ticker, first_seen_date FROM regsho_list
        WHERE collected_date = (SELECT MAX(collected_date) FROM regsho_list WHERE collected_date < CURRENT_DATE)
    """)
    prev_tickers = {row[0]: row[1] for row in cur.fetchall()}

    cur.execute("DELETE FROM regsho_list WHERE collected_date = CURRENT_DATE")
    for item in regSHO:
        ticker = item["ticker"]
        first_seen = prev_tickers.get(ticker, None)
        if first_seen:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, %s)
            """, (ticker, item["name"], item["market"], first_seen))
        else:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, CURRENT_DATE)
            """, (ticker, item["name"], item["market"]))

    # í™˜ìœ¨ ì €ì¥
    cur.execute("""
        INSERT INTO exchange_rates (from_currency, to_currency, rate)
        VALUES ('USD', 'KRW', %s)
    """, (exchange_rate,))

    # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì €ì¥
    for post in blog_posts:
        cur.execute("""
            INSERT INTO blog_posts (post_id, title, content, tickers, keywords, post_date, url, is_new)
            VALUES (%s, %s, %s, %s, %s, %s, %s, TRUE)
            ON CONFLICT (post_id) DO NOTHING
        """, (
            post["post_id"],
            post.get("title"),
            post.get("content"),
            post.get("tickers", []),
            post.get("keywords", []),
            post.get("date"),
            post.get("url")
        ))

    # ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ì €ì¥
    for ticker, info in blogger_tickers.items():
        cur.execute("""
            INSERT INTO blogger_tickers (ticker, ticker_info)
            VALUES (%s, %s)
        """, (ticker, json.dumps(info)))

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì €ì¥ ì™„ë£Œ")


# ============================================================
# ë©”ì¸
# ============================================================

async def main():
    """
    ê°„ì†Œí™”ëœ ìˆ˜ì§‘ê¸° v3
    - ì£¼ê°€: yfinance ì‹¤ì‹œê°„ ì‚¬ìš© (ìˆ˜ì§‘ ë¶ˆí•„ìš”)
    - ìˆìŠ¤í€´ì¦ˆ: deep_analyzerê°€ ì‹¤ì‹œê°„ ë¶„ì„ (ìˆ˜ì§‘ ë¶ˆí•„ìš”)
    - RegSHO: NASDAQì—ì„œ ìˆ˜ì§‘ í•„ìš”
    - í™˜ìœ¨: í•˜ë£¨ 1ë²ˆ ìˆ˜ì§‘
    - ë¸”ë¡œê·¸: ìƒˆ ê¸€ ìŠ¤í¬ë˜í•‘ í•„ìš”
    """
    print("=" * 60)
    print(f"ğŸ“Š Data Collector v3 (Lite) - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    init_db()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # 1. RegSHO ìˆ˜ì§‘
        print("\nğŸ“‹ RegSHO ìˆ˜ì§‘...")
        regSHO = await collect_regSHO(page)

        # 2. í™˜ìœ¨ ìˆ˜ì§‘
        print("\nğŸ’± í™˜ìœ¨ ìˆ˜ì§‘...")
        exchange_rate = await collect_exchange_rate(page)

        # 3. ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)
        print("\nğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)...")
        blog_posts = await collect_blog_posts(page, limit=10)

        # 4. ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘
        all_mentioned = set()
        for post in blog_posts:
            all_mentioned.update(post.get("tickers", []))

        blogger_tickers = {}
        if all_mentioned:
            print("\nğŸ” ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘...")
            blogger_tickers = await collect_blogger_ticker_info(page, list(all_mentioned))

        await browser.close()

    # DB ì €ì¥
    print("\nğŸ’¾ DB ì €ì¥...")
    save_to_db(regSHO, exchange_rate, blog_posts, blogger_tickers)

    # ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"  - RegSHO: {len(regSHO)}ê°œ")
    print(f"  - ìƒˆ ë¸”ë¡œê·¸ ê¸€: {len(blog_posts)}ê°œ")
    print(f"  - ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤: {len(blogger_tickers)}ê°œ")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
