#!/usr/bin/env python3
"""
Stock Briefing Data Collector v2
- Benzingaì—ì„œ ì •í™•í•œ ì£¼ê°€ ìˆ˜ì§‘
- ìƒˆ ë¸”ë¡œê·¸ ê¸€ë§Œ ìˆ˜ì§‘ (DB ë¹„êµ)
- ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
"""

import asyncio
import json
import re
import os
from datetime import datetime
from pathlib import Path

import psycopg2
from psycopg2.extras import RealDictCursor
from playwright.async_api import async_playwright

# ============================================================
# ì„¤ì •
# ============================================================

DB_URL = os.getenv("DATABASE_URL", "postgresql://claude:claude_dev@localhost:5432/continuous_claude")
BLOG_ID = "stock_moonrabbit"

# í¬íŠ¸í´ë¦¬ì˜¤
HOLDINGS = [
    {"ticker": "BNAI", "shares": 464, "avg_cost": 9.55},
    {"ticker": "GLSI", "shares": 67, "avg_cost": 25.22},
]

# ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ (ìˆìŠ¤í€´ì¦ˆ í›„ë³´)
WATCHLIST = [
    {"ticker": "HIMS", "reason": "Short 31%, GLP-1 í…Œë§ˆ, Novo í˜‘ìƒ"},
    {"ticker": "SOUN", "reason": "Short 28%, AI í…Œë§ˆ, 2/26 ì‹¤ì "},
]


# ============================================================
# DB í•¨ìˆ˜
# ============================================================

def get_db():
    return psycopg2.connect(DB_URL)


def init_db():
    """í…Œì´ë¸” ìƒì„±"""
    conn = get_db()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS stock_prices (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            regular_price DECIMAL(10,4),
            afterhours_price DECIMAL(10,4),
            premarket_price DECIMAL(10,4),
            change_percent DECIMAL(8,4),
            source VARCHAR(50),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS regSHO_list (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            security_name TEXT,
            market_category VARCHAR(10),
            collected_date DATE DEFAULT CURRENT_DATE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blog_posts (
            id SERIAL PRIMARY KEY,
            post_id VARCHAR(30) UNIQUE,
            title TEXT,
            content TEXT,
            tickers TEXT[],
            keywords TEXT[],
            post_date VARCHAR(50),
            url TEXT,
            is_new BOOLEAN DEFAULT TRUE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS exchange_rates (
            id SERIAL PRIMARY KEY,
            from_currency VARCHAR(3),
            to_currency VARCHAR(3),
            rate DECIMAL(10,4),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blogger_tickers (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            mentioned_in_post VARCHAR(30),
            ticker_info JSONB,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS stock_briefing (
            id SERIAL PRIMARY KEY,
            briefing_date DATE DEFAULT CURRENT_DATE,
            briefing_json JSONB,
            created_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS ticker_info (
            ticker VARCHAR(10) PRIMARY KEY,
            company_name TEXT,
            sector TEXT,
            industry TEXT,
            updated_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS squeeze_data (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            borrow_rate DECIMAL(10,2),
            short_interest DECIMAL(10,2),
            days_to_cover DECIMAL(8,2),
            short_volume BIGINT,
            squeeze_score DECIMAL(8,2),
            source VARCHAR(50),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE INDEX IF NOT EXISTS idx_stock_prices_ticker ON stock_prices(ticker, collected_at DESC);
        CREATE INDEX IF NOT EXISTS idx_blog_posts_new ON blog_posts(is_new, collected_at DESC);
        CREATE INDEX IF NOT EXISTS idx_squeeze_data_ticker ON squeeze_data(ticker, collected_at DESC);
    """)

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ")


def get_existing_post_ids():
    """ì´ë¯¸ ì €ì¥ëœ í¬ìŠ¤íŠ¸ ID ëª©ë¡"""
    conn = get_db()
    cur = conn.cursor()
    cur.execute("SELECT post_id FROM blog_posts")
    ids = {row[0] for row in cur.fetchall()}
    cur.close()
    conn.close()
    return ids


def mark_posts_as_read():
    """ëª¨ë“  ìƒˆ ê¸€ì„ ì½ìŒ ì²˜ë¦¬"""
    conn = get_db()
    cur = conn.cursor()
    cur.execute("UPDATE blog_posts SET is_new = FALSE WHERE is_new = TRUE")
    conn.commit()
    cur.close()
    conn.close()


def update_ticker_info(tickers):
    """í‹°ì»¤ ì •ë³´(íšŒì‚¬ëª…) ì—…ë°ì´íŠ¸ - yfinance ì‚¬ìš©"""
    import yfinance as yf

    conn = get_db()
    cur = conn.cursor()

    for ticker in tickers:
        try:
            # ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            cur.execute("SELECT ticker FROM ticker_info WHERE ticker = %s", (ticker,))
            if cur.fetchone():
                continue  # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ

            stock = yf.Ticker(ticker)
            info = stock.info

            company_name = info.get("shortName") or info.get("longName") or ticker
            sector = info.get("sector")
            industry = info.get("industry")

            cur.execute("""
                INSERT INTO ticker_info (ticker, company_name, sector, industry, updated_at)
                VALUES (%s, %s, %s, %s, NOW())
                ON CONFLICT (ticker) DO UPDATE SET
                    company_name = EXCLUDED.company_name,
                    sector = EXCLUDED.sector,
                    industry = EXCLUDED.industry,
                    updated_at = NOW()
            """, (ticker, company_name, sector, industry))

            print(f"  âœ… {ticker}: {company_name}")
        except Exception as e:
            print(f"  âŒ {ticker}: {e}")

    conn.commit()
    cur.close()
    conn.close()


# ============================================================
# ì£¼ê°€ ìˆ˜ì§‘ (Benzinga)
# ============================================================

async def collect_stock_prices(page, tickers):
    """ì£¼ê°€ ìˆ˜ì§‘ (yfinance ìš°ì„ , ë” ì•ˆì •ì )"""
    import yfinance as yf

    prices = {}

    for ticker in tickers:
        try:
            stock = yf.Ticker(ticker)
            info = stock.info

            regular = info.get("regularMarketPrice") or info.get("currentPrice")
            premarket = info.get("preMarketPrice")
            afterhours = info.get("postMarketPrice")
            change_pct = info.get("regularMarketChangePercent")

            prices[ticker] = {
                "regular": regular,
                "afterhours": afterhours,
                "premarket": premarket,
                "change_pct": round(change_pct, 2) if change_pct else None,
            }

            current = afterhours or premarket or regular
            print(f"  {ticker}: ì¢…ê°€ ${regular} | AH ${afterhours} | PM ${premarket} | í˜„ì¬ ${current}")

        except Exception as e:
            print(f"  {ticker}: yfinance ì‹¤íŒ¨ - {e}")
            prices[ticker] = {"regular": None, "afterhours": None, "premarket": None, "change_pct": None}

    return prices


# ============================================================
# RegSHO ìˆ˜ì§‘
# ============================================================

async def collect_regSHO(page):
    """RegSHO Threshold List ìˆ˜ì§‘"""
    url = "https://www.nasdaqtrader.com/trader.aspx?id=regshothreshold"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    content = await page.content()

    tickers = []
    # í…Œì´ë¸”ì—ì„œ í‹°ì»¤ ì¶”ì¶œ
    rows = re.findall(r'>([A-Z]{2,5})</td>\s*<td[^>]*>([^<]+)</td>\s*<td[^>]*>([QGS])</td>', content)

    for ticker, name, market in rows:
        tickers.append({
            "ticker": ticker,
            "name": name.strip(),
            "market": market
        })

    print(f"  RegSHO: {len(tickers)}ê°œ ì¢…ëª©")
    return tickers


# ============================================================
# í™˜ìœ¨ ìˆ˜ì§‘
# ============================================================

async def collect_exchange_rate(page):
    """USD/KRW í™˜ìœ¨ ìˆ˜ì§‘"""
    url = "https://www.google.com/finance/quote/USD-KRW"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    text = await page.inner_text("body")

    # í™˜ìœ¨ íŒ¨í„´
    match = re.search(r'(\d{1,3}(?:,\d{3})*\.?\d*)\s*(?:KRW|ì›)', text)
    if match:
        rate = float(match.group(1).replace(',', ''))
    else:
        rate = 1450.0  # ê¸°ë³¸ê°’

    print(f"  í™˜ìœ¨: $1 = â‚©{rate:,.2f}")
    return rate


# ============================================================
# ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (yfinance)
# ============================================================

async def collect_squeeze_data(page, tickers):
    """yfinanceì—ì„œ ìˆìŠ¤í€´ì¦ˆ ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘"""
    import yfinance as yf

    squeeze_data = {}

    for ticker in tickers:
        try:
            stock = yf.Ticker(ticker)
            info = stock.info

            # Short Interest (ìœ ë™ì£¼ì‹ ëŒ€ë¹„ ê³µë§¤ë„ ë¹„ìœ¨, %)
            short_pct = info.get("shortPercentOfFloat")
            short_interest = round(short_pct * 100, 2) if short_pct else None

            # Days to Cover (Short Ratio)
            days_to_cover = info.get("shortRatio")

            # Shares Short
            short_volume = info.get("sharesShort")

            # Float shares for context
            float_shares = info.get("floatShares")

            # ìŠ¤í€´ì¦ˆ ì ìˆ˜ ê³„ì‚° (0-100)
            # borrow_rateëŠ” yfinanceì—ì„œ ì œê³µí•˜ì§€ ì•ŠìŒ
            squeeze_score = calculate_squeeze_score(None, short_interest, days_to_cover)

            squeeze_data[ticker] = {
                "borrow_rate": None,  # yfinance doesn't provide this
                "short_interest": short_interest,
                "days_to_cover": round(days_to_cover, 2) if days_to_cover else None,
                "short_volume": short_volume,
                "squeeze_score": squeeze_score,
            }

            print(f"  {ticker}: SI {short_interest}% | DTC {days_to_cover} | Score {squeeze_score}")

        except Exception as e:
            print(f"  âŒ {ticker}: {e}")
            squeeze_data[ticker] = {
                "borrow_rate": None,
                "short_interest": None,
                "days_to_cover": None,
                "short_volume": None,
                "squeeze_score": None,
            }

    return squeeze_data


def calculate_squeeze_score(borrow_rate, short_interest, days_to_cover):
    """
    ìˆìŠ¤í€´ì¦ˆ í™•ë¥  ì ìˆ˜ ê³„ì‚° (0-100)
    - Short Interest: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (60% ê°€ì¤‘ì¹˜) - borrow rate ì—†ì–´ì„œ ê°€ì¤‘ì¹˜ ì¦ê°€
    - Days to Cover: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (40% ê°€ì¤‘ì¹˜)
    """
    if not any([short_interest, days_to_cover]):
        return None

    score = 0

    # Short Interest ì ìˆ˜ (0-60): 50%+ = ë§Œì , 0% = 0ì 
    if short_interest:
        si_score = min(short_interest * 2, 100) * 0.6
        score += si_score

    # Days to Cover ì ìˆ˜ (0-40): 10ì¼+ = ë§Œì , 0ì¼ = 0ì 
    if days_to_cover:
        dtc_score = min(days_to_cover * 10, 100) * 0.4
        score += dtc_score

    return round(score, 1)


# ============================================================
# ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)
# ============================================================

async def collect_blog_posts(page, limit=10):
    """ìƒˆ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘"""
    existing_ids = get_existing_post_ids()
    print(f"  ê¸°ì¡´ í¬ìŠ¤íŠ¸: {len(existing_ids)}ê°œ")

    # í¬ìŠ¤íŠ¸ ëª©ë¡ í˜ì´ì§€
    list_url = f"https://m.blog.naver.com/PostList.naver?blogId={BLOG_ID}&categoryNo=0"
    await page.goto(list_url)
    await page.wait_for_timeout(2000)

    # ìŠ¤í¬ë¡¤í•´ì„œ ë” ë§ì€ í¬ìŠ¤íŠ¸ ë¡œë“œ
    for _ in range(5):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(500)

    # í¬ìŠ¤íŠ¸ ë§í¬ ìˆ˜ì§‘
    links = await page.query_selector_all('a[href*="logNo"]')

    new_posts = []
    seen = set()

    for link in links:
        try:
            href = await link.get_attribute("href")
            if not href or "logNo=" not in href:
                continue

            post_id = href.split("logNo=")[-1].split("&")[0]

            if post_id in seen or post_id in existing_ids:
                continue
            seen.add(post_id)

            if not href.startswith("http"):
                href = f"https://m.blog.naver.com{href}"

            new_posts.append({
                "post_id": post_id,
                "url": href
            })

            if len(new_posts) >= limit:
                break

        except Exception:
            continue

    print(f"  ìƒˆ í¬ìŠ¤íŠ¸: {len(new_posts)}ê°œ ë°œê²¬")

    # ê° ìƒˆ í¬ìŠ¤íŠ¸ ë‚´ìš© ìˆ˜ì§‘
    for post in new_posts:
        try:
            await page.goto(post["url"])
            await page.wait_for_timeout(2000)

            # ì œëª©
            post["title"] = ""
            for sel in ["h3.se_title", "[class*='title']", "h3"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["title"] = (await elem.inner_text()).strip()[:200]
                    if post["title"]:
                        break

            # ë‚ ì§œ
            post["date"] = ""
            for sel in ["[class*='date']", "time"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["date"] = (await elem.inner_text()).strip()
                    break

            # ë³¸ë¬¸
            post["content"] = ""
            for sel in ["div.se-main-container", "div#postViewArea"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["content"] = (await elem.inner_text()).strip()[:10000]
                    break

            # í‹°ì»¤ ì¶”ì¶œ
            content = post.get("content", "")
            raw_tickers = set(re.findall(r'\b([A-Z]{2,5})\b', content))
            # ì¼ë°˜ ë‹¨ì–´ ì œì™¸
            common = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'SEC', 'ETF', 'CEO', 'IPO', 'FDA', 'NYSE', 'USD', 'KRW', 'SPAC', 'PIPE'}
            post["tickers"] = list(raw_tickers - common)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = []
            kw_map = {
                "ìˆìŠ¤í€´ì¦ˆ": ["ìˆìŠ¤í€´ì¦ˆ", "short squeeze", "ìˆì»¤ë²„"],
                "ê°•ì œì²­ì‚°": ["ê°•ì œì²­ì‚°", "forced buy", "close out"],
                "RegSHO": ["regsho", "reg sho", "threshold"],
                "FTD": ["ftd", "fail to deliver"],
                "ê¸‰ë“±": ["ê¸‰ë“±", "í­ë“±", "ìƒìŠ¹"],
                "ê¸‰ë½": ["ê¸‰ë½", "í­ë½", "í•˜ë½"],
            }
            content_lower = content.lower()
            for kw, patterns in kw_map.items():
                if any(p.lower() in content_lower for p in patterns):
                    keywords.append(kw)
            post["keywords"] = keywords

            print(f"    â†’ {post['title'][:40]}... | í‹°ì»¤: {post['tickers'][:5]}")

        except Exception as e:
            print(f"    â†’ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return new_posts


# ============================================================
# ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
# ============================================================

async def collect_blogger_ticker_info(page, tickers):
    """ë¸”ë¡œê±°ê°€ ì–¸ê¸‰í•œ í‹°ì»¤ë“¤ì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘"""
    ticker_info = {}

    # ë³´ìœ  ì¢…ëª© ì œì™¸, ìƒˆë¡œ ì–¸ê¸‰ëœ í‹°ì»¤ë§Œ
    my_tickers = {h["ticker"] for h in HOLDINGS}
    new_tickers = [t for t in tickers if t not in my_tickers][:5]  # ìµœëŒ€ 5ê°œ

    for ticker in new_tickers:
        try:
            url = f"https://www.benzinga.com/quote/{ticker.lower()}"
            await page.goto(url)
            await page.wait_for_timeout(2000)

            text = await page.inner_text("body")

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            price_match = re.search(r'\$(\d+\.?\d*)', text)
            change_match = re.search(r'([+-]?\d+\.?\d*)\s*%', text)

            ticker_info[ticker] = {
                "price": float(price_match.group(1)) if price_match else None,
                "change_pct": float(change_match.group(1)) if change_match else None,
                "collected_at": datetime.now().isoformat()
            }

            print(f"    {ticker}: ${ticker_info[ticker]['price']} ({ticker_info[ticker]['change_pct']}%)")

        except Exception as e:
            print(f"    {ticker}: ìˆ˜ì§‘ ì‹¤íŒ¨")

    return ticker_info


# ============================================================
# DB ì €ì¥
# ============================================================

def save_to_db(prices, regSHO, exchange_rate, blog_posts, blogger_tickers, squeeze_data=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    conn = get_db()
    cur = conn.cursor()

    # ì£¼ê°€ ì €ì¥
    for ticker, data in prices.items():
        cur.execute("""
            INSERT INTO stock_prices (ticker, regular_price, afterhours_price, premarket_price, change_percent, source)
            VALUES (%s, %s, %s, %s, %s, 'benzinga')
        """, (ticker, data.get("regular"), data.get("afterhours"), data.get("premarket"), data.get("change_pct")))

    # RegSHO ì €ì¥ (ì—°ì† ë“±ì¬ì¼ ì¶”ì )
    # ì–´ì œ ë“±ì¬ëœ í‹°ì»¤ì™€ first_seen_date ê°€ì ¸ì˜¤ê¸°
    cur.execute("""
        SELECT ticker, first_seen_date FROM regsho_list
        WHERE collected_date = (SELECT MAX(collected_date) FROM regsho_list WHERE collected_date < CURRENT_DATE)
    """)
    prev_tickers = {row[0]: row[1] for row in cur.fetchall()}

    # ì˜¤ëŠ˜ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
    cur.execute("DELETE FROM regsho_list WHERE collected_date = CURRENT_DATE")
    for item in regSHO:
        ticker = item["ticker"]
        # ì–´ì œë„ ìˆì—ˆìœ¼ë©´ first_seen_date ìœ ì§€, ì•„ë‹ˆë©´ ì˜¤ëŠ˜ ë‚ ì§œ
        first_seen = prev_tickers.get(ticker, None)
        if first_seen:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, %s)
            """, (ticker, item["name"], item["market"], first_seen))
        else:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, CURRENT_DATE)
            """, (ticker, item["name"], item["market"]))

    # í™˜ìœ¨ ì €ì¥
    cur.execute("""
        INSERT INTO exchange_rates (from_currency, to_currency, rate)
        VALUES ('USD', 'KRW', %s)
    """, (exchange_rate,))

    # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì €ì¥ (ìƒˆ ê¸€ë§Œ)
    for post in blog_posts:
        cur.execute("""
            INSERT INTO blog_posts (post_id, title, content, tickers, keywords, post_date, url, is_new)
            VALUES (%s, %s, %s, %s, %s, %s, %s, TRUE)
            ON CONFLICT (post_id) DO NOTHING
        """, (
            post["post_id"],
            post.get("title"),
            post.get("content"),
            post.get("tickers", []),
            post.get("keywords", []),
            post.get("date"),
            post.get("url")
        ))

    # ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ì €ì¥
    for ticker, info in blogger_tickers.items():
        cur.execute("""
            INSERT INTO blogger_tickers (ticker, ticker_info)
            VALUES (%s, %s)
        """, (ticker, json.dumps(info)))

    # ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ì €ì¥
    if squeeze_data:
        for ticker, data in squeeze_data.items():
            if data.get("squeeze_score") is not None:
                cur.execute("""
                    INSERT INTO squeeze_data (ticker, borrow_rate, short_interest, days_to_cover, short_volume, squeeze_score, source)
                    VALUES (%s, %s, %s, %s, %s, %s, 'chartexchange')
                """, (
                    ticker,
                    data.get("borrow_rate"),
                    data.get("short_interest"),
                    data.get("days_to_cover"),
                    data.get("short_volume"),
                    data.get("squeeze_score")
                ))

    # ë¸Œë¦¬í•‘ JSON ìƒì„± ë° ì €ì¥
    briefing = generate_briefing(prices, regSHO, exchange_rate, blog_posts, blogger_tickers)
    cur.execute("DELETE FROM stock_briefing WHERE briefing_date = CURRENT_DATE")
    cur.execute("""
        INSERT INTO stock_briefing (briefing_date, briefing_json)
        VALUES (CURRENT_DATE, %s)
    """, (json.dumps(briefing, ensure_ascii=False),))

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì €ì¥ ì™„ë£Œ")


def generate_briefing(prices, regSHO, exchange_rate, blog_posts, blogger_tickers):
    """ë¸Œë¦¬í•‘ JSON ìƒì„±"""
    # í¬íŠ¸í´ë¦¬ì˜¤ ê³„ì‚°
    portfolio = []
    total_value = 0
    total_cost = 0

    for holding in HOLDINGS:
        ticker = holding["ticker"]
        data = prices.get(ticker, {})

        # ìš°ì„ ìˆœìœ„: ì• í”„í„° > í”„ë¦¬ > ì¢…ê°€
        current = data.get("afterhours") or data.get("premarket") or data.get("regular") or 0

        value = holding["shares"] * current
        cost = holding["shares"] * holding["avg_cost"]
        gain = value - cost
        gain_pct = (gain / cost * 100) if cost > 0 else 0

        portfolio.append({
            "ticker": ticker,
            "shares": holding["shares"],
            "avg_cost": holding["avg_cost"],
            "regular_price": data.get("regular"),
            "afterhours_price": data.get("afterhours"),
            "premarket_price": data.get("premarket"),
            "current_price": current,
            "value": round(value, 2),
            "gain": round(gain, 2),
            "gain_pct": round(gain_pct, 1)
        })

        total_value += value
        total_cost += cost

    total_gain = total_value - total_cost
    total_gain_krw = total_gain * exchange_rate

    # ì„¸ê¸ˆ ê³„ì‚°
    taxable = max(0, total_gain_krw - 2500000)
    tax = taxable * 0.22
    net_profit = total_gain_krw - tax

    # RegSHO ì²´í¬
    regSHO_tickers = {item["ticker"] for item in regSHO}
    holdings_in_regSHO = [h["ticker"] for h in HOLDINGS if h["ticker"] in regSHO_tickers]

    # ìƒˆ ë¸”ë¡œê·¸ ê¸€ ìš”ì•½
    new_blog_posts = []
    all_blog_tickers = set()
    for post in blog_posts:
        new_blog_posts.append({
            "title": post.get("title", "")[:100],
            "url": post.get("url"),
            "tickers": post.get("tickers", [])[:10],
            "keywords": post.get("keywords", []),
            "date": post.get("date")
        })
        all_blog_tickers.update(post.get("tickers", []))

    return {
        "timestamp": datetime.now().isoformat(),
        "exchange_rate": exchange_rate,
        "portfolio": portfolio,
        "total": {
            "value_usd": round(total_value, 2),
            "value_krw": round(total_value * exchange_rate, 0),
            "gain_usd": round(total_gain, 2),
            "gain_krw": round(total_gain_krw, 0),
            "gain_pct": round((total_gain / total_cost * 100) if total_cost > 0 else 0, 1)
        },
        "tax": {
            "taxable_krw": round(taxable, 0),
            "tax_krw": round(tax, 0),
            "net_profit_krw": round(net_profit, 0)
        },
        "regSHO": {
            "total_count": len(regSHO),
            "holdings_on_list": holdings_in_regSHO,
            "top_tickers": [item["ticker"] for item in regSHO[:15]]
        },
        "new_blog_posts": new_blog_posts,
        "blogger_mentioned_tickers": list(all_blog_tickers),
        "blogger_ticker_info": blogger_tickers
    }


# ============================================================
# ë©”ì¸
# ============================================================

async def main():
    print("=" * 60)
    print(f"ğŸ“Š Stock Data Collector v2 - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    init_db()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # 1. ì£¼ê°€ ìˆ˜ì§‘ (í¬íŠ¸í´ë¦¬ì˜¤ + ì›Œì¹˜ë¦¬ìŠ¤íŠ¸)
        print("\nğŸ“ˆ ì£¼ê°€ ìˆ˜ì§‘...")
        tickers = [h["ticker"] for h in HOLDINGS] + [w["ticker"] for w in WATCHLIST]
        prices = await collect_stock_prices(page, tickers)

        # 2. RegSHO ìˆ˜ì§‘
        print("\nğŸ“‹ RegSHO ìˆ˜ì§‘...")
        regSHO = await collect_regSHO(page)

        # 3. í™˜ìœ¨ ìˆ˜ì§‘
        print("\nğŸ’± í™˜ìœ¨ ìˆ˜ì§‘...")
        exchange_rate = await collect_exchange_rate(page)

        # 4. ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)
        print("\nğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)...")
        blog_posts = await collect_blog_posts(page, limit=10)

        # 5. ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘
        all_mentioned = set()
        for post in blog_posts:
            all_mentioned.update(post.get("tickers", []))

        blogger_tickers = {}
        if all_mentioned:
            print("\nğŸ” ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘...")
            blogger_tickers = await collect_blogger_ticker_info(page, list(all_mentioned))

        # 6. ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (í¬íŠ¸í´ë¦¬ì˜¤ + RegSHO ì¢…ëª©)
        print("\nğŸ”¥ ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘...")
        squeeze_tickers = list(set(tickers + [r["ticker"] for r in regSHO]))
        squeeze_data = await collect_squeeze_data(page, squeeze_tickers)

        await browser.close()

    # DB ì €ì¥
    print("\nğŸ’¾ DB ì €ì¥...")
    save_to_db(prices, regSHO, exchange_rate, blog_posts, blogger_tickers, squeeze_data)

    # í‹°ì»¤ ì •ë³´(íšŒì‚¬ëª…) ì—…ë°ì´íŠ¸
    print("\nğŸ“› í‹°ì»¤ ì •ë³´ ì—…ë°ì´íŠ¸...")
    update_ticker_info(tickers)

    # ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"  - ì£¼ê°€: {len(prices)}ê°œ")
    print(f"  - RegSHO: {len(regSHO)}ê°œ")
    print(f"  - ìƒˆ ë¸”ë¡œê·¸ ê¸€: {len(blog_posts)}ê°œ")
    print(f"  - ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤: {len(blogger_tickers)}ê°œ")
    print(f"  - ìˆìŠ¤í€´ì¦ˆ ë°ì´í„°: {len(squeeze_data)}ê°œ")
    print("=" * 60)

    # í‘¸ì‹œ ì•Œë¦¼ ë°œì†¡
    print("\nğŸ”” í‘¸ì‹œ ì•Œë¦¼ ë°œì†¡...")
    try:
        from api.notifications import send_data_update_notification
        result = send_data_update_notification()
        print(f"  - ë°œì†¡: {result.get('sent', 0)}ê±´, ë§Œë£Œ ì‚­ì œ: {result.get('expired', 0)}ê±´")
    except Exception as e:
        print(f"  - ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    asyncio.run(main())
