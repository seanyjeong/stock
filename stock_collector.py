#!/usr/bin/env python3
"""
Stock Briefing Data Collector v2
- Benzingaì—ì„œ ì •í™•í•œ ì£¼ê°€ ìˆ˜ì§‘
- ìƒˆ ë¸”ë¡œê·¸ ê¸€ë§Œ ìˆ˜ì§‘ (DB ë¹„êµ)
- ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
"""

import asyncio
import json
import re
import os
from datetime import datetime
from pathlib import Path

import psycopg2
from psycopg2.extras import RealDictCursor
from playwright.async_api import async_playwright

# ============================================================
# ì„¤ì •
# ============================================================

DB_URL = os.getenv("DATABASE_URL", "postgresql://claude:claude_dev@localhost:5432/continuous_claude")
BLOG_ID = "stock_moonrabbit"

# í¬íŠ¸í´ë¦¬ì˜¤
HOLDINGS = [
    {"ticker": "BNAI", "shares": 464, "avg_cost": 9.55},
    {"ticker": "GLSI", "shares": 67, "avg_cost": 25.22},
]

# ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ (ìˆìŠ¤í€´ì¦ˆ í›„ë³´)
WATCHLIST = [
    {"ticker": "HIMS", "reason": "Short 31%, GLP-1 í…Œë§ˆ, Novo í˜‘ìƒ"},
    {"ticker": "SOUN", "reason": "Short 28%, AI í…Œë§ˆ, 2/26 ì‹¤ì "},
]


# ============================================================
# DB í•¨ìˆ˜
# ============================================================

def get_db():
    return psycopg2.connect(DB_URL)


def init_db():
    """í…Œì´ë¸” ìƒì„±"""
    conn = get_db()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS stock_prices (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            regular_price DECIMAL(10,4),
            afterhours_price DECIMAL(10,4),
            premarket_price DECIMAL(10,4),
            change_percent DECIMAL(8,4),
            source VARCHAR(50),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS regSHO_list (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            security_name TEXT,
            market_category VARCHAR(10),
            collected_date DATE DEFAULT CURRENT_DATE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blog_posts (
            id SERIAL PRIMARY KEY,
            post_id VARCHAR(30) UNIQUE,
            title TEXT,
            content TEXT,
            tickers TEXT[],
            keywords TEXT[],
            post_date VARCHAR(50),
            url TEXT,
            is_new BOOLEAN DEFAULT TRUE,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS exchange_rates (
            id SERIAL PRIMARY KEY,
            from_currency VARCHAR(3),
            to_currency VARCHAR(3),
            rate DECIMAL(10,4),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS blogger_tickers (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            mentioned_in_post VARCHAR(30),
            ticker_info JSONB,
            collected_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS stock_briefing (
            id SERIAL PRIMARY KEY,
            briefing_date DATE DEFAULT CURRENT_DATE,
            briefing_json JSONB,
            created_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS ticker_info (
            ticker VARCHAR(10) PRIMARY KEY,
            company_name TEXT,
            sector TEXT,
            industry TEXT,
            updated_at TIMESTAMP DEFAULT NOW()
        );

        CREATE TABLE IF NOT EXISTS squeeze_data (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            borrow_rate DECIMAL(10,2),
            short_interest DECIMAL(10,2),
            days_to_cover DECIMAL(8,2),
            short_volume BIGINT,
            squeeze_score DECIMAL(8,2),
            available_shares BIGINT,
            float_shares BIGINT,
            dilution_protected BOOLEAN DEFAULT FALSE,
            has_positive_news BOOLEAN DEFAULT FALSE,
            has_negative_news BOOLEAN DEFAULT FALSE,
            source VARCHAR(50),
            collected_at TIMESTAMP DEFAULT NOW()
        );

        -- v2 ì»¬ëŸ¼ ì¶”ê°€ (ì´ë¯¸ í…Œì´ë¸” ìˆìœ¼ë©´)
        DO $$
        BEGIN
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='squeeze_data' AND column_name='available_shares') THEN
                ALTER TABLE squeeze_data ADD COLUMN available_shares BIGINT;
            END IF;
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='squeeze_data' AND column_name='float_shares') THEN
                ALTER TABLE squeeze_data ADD COLUMN float_shares BIGINT;
            END IF;
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='squeeze_data' AND column_name='dilution_protected') THEN
                ALTER TABLE squeeze_data ADD COLUMN dilution_protected BOOLEAN DEFAULT FALSE;
            END IF;
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='squeeze_data' AND column_name='has_positive_news') THEN
                ALTER TABLE squeeze_data ADD COLUMN has_positive_news BOOLEAN DEFAULT FALSE;
            END IF;
            IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name='squeeze_data' AND column_name='has_negative_news') THEN
                ALTER TABLE squeeze_data ADD COLUMN has_negative_news BOOLEAN DEFAULT FALSE;
            END IF;
        END $$;

        CREATE INDEX IF NOT EXISTS idx_stock_prices_ticker ON stock_prices(ticker, collected_at DESC);
        CREATE INDEX IF NOT EXISTS idx_blog_posts_new ON blog_posts(is_new, collected_at DESC);
        CREATE INDEX IF NOT EXISTS idx_squeeze_data_ticker ON squeeze_data(ticker, collected_at DESC);
    """)

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì´ˆê¸°í™” ì™„ë£Œ")


def get_existing_post_ids():
    """ì´ë¯¸ ì €ì¥ëœ í¬ìŠ¤íŠ¸ ID ëª©ë¡"""
    conn = get_db()
    cur = conn.cursor()
    cur.execute("SELECT post_id FROM blog_posts")
    ids = {row[0] for row in cur.fetchall()}
    cur.close()
    conn.close()
    return ids


def mark_posts_as_read():
    """ëª¨ë“  ìƒˆ ê¸€ì„ ì½ìŒ ì²˜ë¦¬"""
    conn = get_db()
    cur = conn.cursor()
    cur.execute("UPDATE blog_posts SET is_new = FALSE WHERE is_new = TRUE")
    conn.commit()
    cur.close()
    conn.close()


def update_ticker_info(tickers):
    """í‹°ì»¤ ì •ë³´(íšŒì‚¬ëª…) ì—…ë°ì´íŠ¸ - yfinance ì‚¬ìš©"""
    import yfinance as yf

    conn = get_db()
    cur = conn.cursor()

    for ticker in tickers:
        try:
            # ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            cur.execute("SELECT ticker FROM ticker_info WHERE ticker = %s", (ticker,))
            if cur.fetchone():
                continue  # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ

            stock = yf.Ticker(ticker)
            info = stock.info

            company_name = info.get("shortName") or info.get("longName") or ticker
            sector = info.get("sector")
            industry = info.get("industry")

            cur.execute("""
                INSERT INTO ticker_info (ticker, company_name, sector, industry, updated_at)
                VALUES (%s, %s, %s, %s, NOW())
                ON CONFLICT (ticker) DO UPDATE SET
                    company_name = EXCLUDED.company_name,
                    sector = EXCLUDED.sector,
                    industry = EXCLUDED.industry,
                    updated_at = NOW()
            """, (ticker, company_name, sector, industry))

            print(f"  âœ… {ticker}: {company_name}")
        except Exception as e:
            print(f"  âŒ {ticker}: {e}")

    conn.commit()
    cur.close()
    conn.close()


# ============================================================
# ì£¼ê°€ ìˆ˜ì§‘ (Benzinga)
# ============================================================

async def collect_stock_prices(page, tickers):
    """ì£¼ê°€ ìˆ˜ì§‘ (yfinance ìš°ì„ , ë” ì•ˆì •ì )"""
    import yfinance as yf

    prices = {}

    for ticker in tickers:
        try:
            stock = yf.Ticker(ticker)
            info = stock.info

            regular = info.get("regularMarketPrice") or info.get("currentPrice")
            premarket = info.get("preMarketPrice")
            afterhours = info.get("postMarketPrice")
            change_pct = info.get("regularMarketChangePercent")

            prices[ticker] = {
                "regular": regular,
                "afterhours": afterhours,
                "premarket": premarket,
                "change_pct": round(change_pct, 2) if change_pct else None,
            }

            current = afterhours or premarket or regular
            print(f"  {ticker}: ì¢…ê°€ ${regular} | AH ${afterhours} | PM ${premarket} | í˜„ì¬ ${current}")

        except Exception as e:
            print(f"  {ticker}: yfinance ì‹¤íŒ¨ - {e}")
            prices[ticker] = {"regular": None, "afterhours": None, "premarket": None, "change_pct": None}

    return prices


# ============================================================
# RegSHO ìˆ˜ì§‘
# ============================================================

async def collect_regSHO(page):
    """RegSHO Threshold List ìˆ˜ì§‘"""
    url = "https://www.nasdaqtrader.com/trader.aspx?id=regshothreshold"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    content = await page.content()

    tickers = []
    # í…Œì´ë¸”ì—ì„œ í‹°ì»¤ ì¶”ì¶œ
    rows = re.findall(r'>([A-Z]{2,5})</td>\s*<td[^>]*>([^<]+)</td>\s*<td[^>]*>([QGS])</td>', content)

    for ticker, name, market in rows:
        tickers.append({
            "ticker": ticker,
            "name": name.strip(),
            "market": market
        })

    print(f"  RegSHO: {len(tickers)}ê°œ ì¢…ëª©")
    return tickers


# ============================================================
# í™˜ìœ¨ ìˆ˜ì§‘
# ============================================================

async def collect_exchange_rate(page):
    """USD/KRW í™˜ìœ¨ ìˆ˜ì§‘"""
    url = "https://www.google.com/finance/quote/USD-KRW"
    await page.goto(url)
    await page.wait_for_timeout(2000)

    text = await page.inner_text("body")

    # í™˜ìœ¨ íŒ¨í„´
    match = re.search(r'(\d{1,3}(?:,\d{3})*\.?\d*)\s*(?:KRW|ì›)', text)
    if match:
        rate = float(match.group(1).replace(',', ''))
    else:
        rate = 1450.0  # ê¸°ë³¸ê°’

    print(f"  í™˜ìœ¨: $1 = â‚©{rate:,.2f}")
    return rate


# ============================================================
# Borrow Rate ìˆ˜ì§‘ (shortablestocks.com - ë¬´ë£Œ)
# ============================================================

async def collect_borrow_rates(page, tickers):
    """shortablestocks.comì—ì„œ Borrow ì •ë³´ ìˆ˜ì§‘"""
    from playwright_stealth import Stealth

    borrow_data = {}

    # Stealth ì ìš© (ë´‡ ê°ì§€ ìš°íšŒ)
    stealth = Stealth()
    await stealth.apply_stealth_async(page.context)

    for ticker in tickers:
        try:
            url = f"https://www.shortablestocks.com/?{ticker}"
            await page.goto(url, timeout=20000)
            await page.wait_for_timeout(3000)

            text = await page.inner_text("body")

            # "Zero Borrow" = ë¹Œë¦´ ì£¼ì‹ ì—†ìŒ (ê·¹ë„ë¡œ ë†’ì€ borrow rate)
            is_zero_borrow = "zero borrow" in text.lower()

            # Short Interest ë°ì´í„° ì¶”ì¶œ
            # íŒ¨í„´: "12/31/2025	1,230,242	14,571,178	1"
            si_match = re.search(r'\d{1,2}/\d{1,2}/\d{4}\s+([\d,]+)\s+([\d,]+)\s+(\d+)', text)

            short_interest_shares = None
            avg_volume = None
            days_to_cover_official = None

            if si_match:
                short_interest_shares = int(si_match.group(1).replace(',', ''))
                avg_volume = int(si_match.group(2).replace(',', ''))
                days_to_cover_official = int(si_match.group(3))

            # Zero Borrow = 999% (ê·¹ë„ë¡œ ë†’ìŒ), ì•„ë‹ˆë©´ None
            borrow_rate = 999.0 if is_zero_borrow else None

            available = 0 if is_zero_borrow else None
            borrow_data[ticker] = {
                "borrow_rate": borrow_rate,
                "available_shares": available,
                "short_interest_shares": short_interest_shares,
                "is_zero_borrow": is_zero_borrow,
            }

            if is_zero_borrow:
                print(f"  ğŸ”¥ {ticker}: ZERO BORROW! (ë¹Œë¦´ ì£¼ì‹ ì—†ìŒ)")
            elif short_interest_shares:
                print(f"  {ticker}: SI {short_interest_shares:,}ì£¼")
            else:
                print(f"  {ticker}: ë°ì´í„° ì—†ìŒ")

        except Exception as e:
            print(f"  âŒ {ticker}: {e}")
            borrow_data[ticker] = {
                "borrow_rate": None,
                "available_shares": None,
                "short_interest_shares": None,
                "is_zero_borrow": False,
            }

    return borrow_data


# ============================================================
# SEC EDGAR ì›ŒëŸ°íŠ¸/í¬ì„ ì •ë³´ (Full-Text Search API)
# ============================================================

async def collect_sec_dilution_info(tickers):
    """SEC EDGAR Full-Text Searchë¡œ ì›ŒëŸ°íŠ¸/í¬ì„ ê´€ë ¨ ì •ë³´ ìˆ˜ì§‘"""
    import httpx

    dilution_data = {}
    headers = {"User-Agent": "DailyStockStory/1.0 (contact@example.com)"}

    async with httpx.AsyncClient(timeout=30) as client:
        for ticker in tickers:
            try:
                found_info = {
                    "warrant_mentions": 0,
                    "dilution_mentions": 0,
                    "covenant_mentions": 0,
                    "positive_news": 0,  # í˜¸ì¬ ë‰´ìŠ¤
                    "negative_news": 0,  # ì•…ì¬ ë‰´ìŠ¤
                }

                # SEC Full-Text Search: í‹°ì»¤ + í‚¤ì›Œë“œë¡œ ì§ì ‘ ê²€ìƒ‰
                keywords_to_search = [
                    ("warrant", "warrant_mentions"),
                    ("dilution", "dilution_mentions"),
                    ("covenant", "covenant_mentions"),
                ]

                # í˜¸ì¬ í‚¤ì›Œë“œ (2025ë…„ ì´í›„ë§Œ)
                positive_keywords = ["deal", "partnership", "contract", "agreement"]

                for keyword, field in keywords_to_search:
                    search_url = f'https://efts.sec.gov/LATEST/search-index?q="{keyword}" AND "{ticker}"&dateRange=custom&startdt=2024-01-01'
                    resp = await client.get(search_url, headers=headers)

                    if resp.status_code == 200:
                        data = resp.json()
                        count = data.get("hits", {}).get("total", {}).get("value", 0)
                        found_info[field] = count

                # í˜¸ì¬ ê³µì‹œ ê²€ìƒ‰ (2025ë…„)
                for pk in positive_keywords:
                    search_url = f'https://efts.sec.gov/LATEST/search-index?q="{pk}" AND "{ticker}"&dateRange=custom&startdt=2025-01-01'
                    resp = await client.get(search_url, headers=headers)
                    if resp.status_code == 200:
                        count = resp.json().get("hits", {}).get("total", {}).get("value", 0)
                        found_info["positive_news"] += count

                # ì•…ì¬ í‚¤ì›Œë“œ ê²€ìƒ‰ (2025ë…„)
                negative_keywords = ["lawsuit", "bankruptcy", "default", "fraud", "investigation", "delisting"]
                for nk in negative_keywords:
                    search_url = f'https://efts.sec.gov/LATEST/search-index?q="{nk}" AND "{ticker}"&dateRange=custom&startdt=2025-01-01'
                    resp = await client.get(search_url, headers=headers)
                    if resp.status_code == 200:
                        count = resp.json().get("hits", {}).get("total", {}).get("value", 0)
                        found_info["negative_news"] += count

                # ê²°ê³¼ í•´ì„
                has_warrant = found_info["warrant_mentions"] > 10
                has_dilution = found_info["dilution_mentions"] > 5
                has_covenant = found_info["covenant_mentions"] > 3
                has_positive_news = found_info["positive_news"] > 50  # 50ê±´ ì´ìƒ = í˜¸ì¬
                has_negative_news = found_info["negative_news"] > 20  # 20ê±´ ì´ìƒ = ì•…ì¬

                dilution_data[ticker] = {
                    "has_warrant_info": has_warrant,
                    "has_debt_covenant": has_covenant,
                    "dilution_risk": has_dilution,
                    "has_positive_news": has_positive_news,
                    "has_negative_news": has_negative_news,
                    "positive_news_count": found_info["positive_news"],
                    "negative_news_count": found_info["negative_news"],
                    "warrant_mentions": found_info["warrant_mentions"],
                    "dilution_mentions": found_info["dilution_mentions"],
                    "covenant_mentions": found_info["covenant_mentions"],
                }

                # ì¶œë ¥
                details = []
                if has_warrant:
                    details.append(f"ì›ŒëŸ°íŠ¸({found_info['warrant_mentions']}ê±´)")
                if has_dilution:
                    details.append(f"í¬ì„({found_info['dilution_mentions']}ê±´)")
                if has_covenant:
                    details.append(f"covenant({found_info['covenant_mentions']}ê±´)")
                if has_positive_news:
                    details.append(f"í˜¸ì¬({found_info['positive_news']}ê±´)ğŸ”¥")
                if has_negative_news:
                    details.append(f"ì•…ì¬({found_info['negative_news']}ê±´)âš ï¸")

                if details:
                    print(f"  ğŸ” {ticker}: {', '.join(details)}")
                else:
                    print(f"  {ticker}: SEC íŠ¹ì´ì‚¬í•­ ì—†ìŒ")

            except Exception as e:
                print(f"  âŒ {ticker} SEC: {e}")
                dilution_data[ticker] = {
                    "has_warrant_info": False,
                    "has_debt_covenant": False,
                    "dilution_risk": False,
                    "has_positive_news": False,
                    "has_negative_news": False,
                    "positive_news_count": 0,
                    "negative_news_count": 0,
                    "warrant_mentions": 0,
                    "dilution_mentions": 0,
                    "covenant_mentions": 0,
                }

    return dilution_data


# ============================================================
# ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (yfinance + Chartexchange)
# ============================================================

async def collect_squeeze_data(page, tickers):
    """ìˆìŠ¤í€´ì¦ˆ ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘ (v2: yfinance + Borrow Rate + SEC)"""
    import yfinance as yf

    # RegSHO + ë³´ìœ ì¢…ëª© ì „ì²´ ìˆ˜ì§‘ (cronì´ë‹ˆê¹Œ ì‹œê°„ OK)
    print(f"  ğŸ“Š Borrow Rate ìˆ˜ì§‘ ({len(tickers)}ê°œ ì¢…ëª©)...")
    borrow_data = await collect_borrow_rates(page, tickers)

    # SEC í¬ì„ ì •ë³´ëŠ” ë³´ìœ  ì¢…ëª©ë§Œ (API ìš”ì²­ ì œí•œ)
    priority_tickers = [h["ticker"] for h in HOLDINGS] + [w["ticker"] for w in WATCHLIST]
    print(f"  ğŸ“‹ SEC í¬ì„ ì •ë³´ ìˆ˜ì§‘ ({len(priority_tickers)}ê°œ ë³´ìœ /ì›Œì¹˜ ì¢…ëª©)...")
    dilution_data = await collect_sec_dilution_info(priority_tickers)

    # 3. yfinance ë°ì´í„° + í†µí•©
    print("  ğŸ“ˆ yfinance ë°ì´í„° ìˆ˜ì§‘...")
    squeeze_data = {}

    for ticker in tickers:
        try:
            stock = yf.Ticker(ticker)
            info = stock.info

            # Short Interest (ìœ ë™ì£¼ì‹ ëŒ€ë¹„ ê³µë§¤ë„ ë¹„ìœ¨, %)
            short_pct = info.get("shortPercentOfFloat")
            short_interest = round(short_pct * 100, 2) if short_pct else None

            # Days to Cover (Short Ratio)
            days_to_cover = info.get("shortRatio")

            # Shares Short
            short_volume = info.get("sharesShort")

            # Float shares for context
            float_shares = info.get("floatShares")

            # Borrow Rate & SEC ì •ë³´ í†µí•©
            borrow_info = borrow_data.get(ticker, {})
            dilution_info = dilution_data.get(ticker, {})
            borrow_rate = borrow_info.get("borrow_rate")
            available_shares = borrow_info.get("available_shares")

            # ìŠ¤í€´ì¦ˆ ì ìˆ˜ v2 ê³„ì‚° (0-100)
            squeeze_score = calculate_squeeze_score_v2(
                borrow_rate=borrow_rate,
                short_interest=short_interest,
                days_to_cover=days_to_cover,
                available_shares=available_shares,
                has_warrant_info=dilution_info.get("has_warrant_info", False),
                has_debt_covenant=dilution_info.get("has_debt_covenant", False),
                float_shares=float_shares,
                has_positive_news=dilution_info.get("has_positive_news", False),
                has_negative_news=dilution_info.get("has_negative_news", False),
            )

            squeeze_data[ticker] = {
                "borrow_rate": borrow_rate,
                "short_interest": short_interest,
                "days_to_cover": round(days_to_cover, 2) if days_to_cover else None,
                "short_volume": short_volume,
                "squeeze_score": squeeze_score,
                "available_shares": available_shares,
                "float_shares": float_shares,
                "dilution_protected": dilution_info.get("has_warrant_info") or dilution_info.get("has_debt_covenant"),
                "has_positive_news": dilution_info.get("has_positive_news", False),
                "has_negative_news": dilution_info.get("has_negative_news", False),
            }

            print(f"  {ticker}: SI {short_interest}% | BR {borrow_rate}% | Score {squeeze_score}")

        except Exception as e:
            print(f"  âŒ {ticker}: {e}")
            squeeze_data[ticker] = {
                "borrow_rate": None,
                "short_interest": None,
                "days_to_cover": None,
                "short_volume": None,
                "squeeze_score": None,
                "available_shares": None,
                "float_shares": None,
                "dilution_protected": False,
                "has_positive_news": False,
                "has_negative_news": False,
            }

    return squeeze_data


def calculate_squeeze_score_v2(borrow_rate, short_interest, days_to_cover,
                               available_shares=None, has_warrant_info=False,
                               has_debt_covenant=False, float_shares=None,
                               has_positive_news=False, has_negative_news=False):
    """
    ìˆìŠ¤í€´ì¦ˆ í™•ë¥  ì ìˆ˜ v2 (0-100)

    Base Score (0-60):
      - Short Interest: 0-25ì  (50%+ = ë§Œì )
      - Borrow Rate: 0-20ì  (200%+ = ë§Œì )
      - Days to Cover: 0-15ì  (10ì¼+ = ë§Œì )

    Squeeze Pressure Bonus (0-25):
      - No shares available: +10ì 
      - Low float (<10M): +5ì 
      - Warrant/debt protection: +10ì 

    Catalyst Bonus (0-10):
      - Positive news/filings: +10ì 

    Risk Penalty (-15):
      - Negative news (lawsuit/bankruptcy/fraud): -15ì 

    Urgency Bonus (0-15):
      - Very high borrow rate (>300%): +10ì 
      - Extreme short interest (>40%): +5ì 
    """
    if not any([borrow_rate, short_interest, days_to_cover]):
        return None

    score = 0

    # === Base Score (0-60) ===

    # Short Interest ì ìˆ˜ (0-25): 50%+ = ë§Œì 
    if short_interest:
        si_score = min(short_interest / 50 * 25, 25)
        score += si_score

    # Borrow Rate ì ìˆ˜ (0-20): 200%+ = ë§Œì 
    if borrow_rate:
        br_score = min(borrow_rate / 200 * 20, 20)
        score += br_score

    # Days to Cover ì ìˆ˜ (0-15): 10ì¼+ = ë§Œì 
    if days_to_cover:
        dtc_score = min(days_to_cover / 10 * 15, 15)
        score += dtc_score

    # === Squeeze Pressure Bonus (0-25) ===

    # No shares available = ì‡¼í‹° ë¹Œë¦´ ì£¼ì‹ ì—†ìŒ
    if available_shares is not None and available_shares == 0:
        score += 10

    # Low float = ì‘ì€ ìœ ë™ì£¼ì‹ìˆ˜
    if float_shares and float_shares < 10_000_000:
        score += 5

    # Warrant/Debt protection = í¬ì„ ë°©ì–´
    if has_warrant_info or has_debt_covenant:
        score += 10

    # === Catalyst Bonus (0-10) ===

    # Positive news/filings = í˜¸ì¬ ê³µì‹œ
    if has_positive_news:
        score += 10

    # === Risk Penalty (-15) ===

    # Negative news = ì•…ì¬ (ì†Œì†¡, íŒŒì‚°, ì‚¬ê¸° ë“±)
    if has_negative_news:
        score -= 15

    # === Urgency Bonus (0-15) ===

    # Very high borrow rate
    if borrow_rate and borrow_rate > 300:
        score += 10

    # Extreme short interest
    if short_interest and short_interest > 40:
        score += 5

    return round(min(score, 100), 1)


# v1 í˜¸í™˜ìš© (deprecated)
def calculate_squeeze_score(borrow_rate, short_interest, days_to_cover):
    """Legacy v1 score - use calculate_squeeze_score_v2 instead"""
    return calculate_squeeze_score_v2(borrow_rate, short_interest, days_to_cover)


# ============================================================
# ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)
# ============================================================

async def collect_blog_posts(page, limit=10):
    """ìƒˆ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë§Œ ìˆ˜ì§‘"""
    existing_ids = get_existing_post_ids()
    print(f"  ê¸°ì¡´ í¬ìŠ¤íŠ¸: {len(existing_ids)}ê°œ")

    # í¬ìŠ¤íŠ¸ ëª©ë¡ í˜ì´ì§€
    list_url = f"https://m.blog.naver.com/PostList.naver?blogId={BLOG_ID}&categoryNo=0"
    await page.goto(list_url)
    await page.wait_for_timeout(2000)

    # ìŠ¤í¬ë¡¤í•´ì„œ ë” ë§ì€ í¬ìŠ¤íŠ¸ ë¡œë“œ
    for _ in range(5):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await page.wait_for_timeout(500)

    # í¬ìŠ¤íŠ¸ ë§í¬ ìˆ˜ì§‘
    links = await page.query_selector_all('a[href*="logNo"]')

    new_posts = []
    seen = set()

    for link in links:
        try:
            href = await link.get_attribute("href")
            if not href or "logNo=" not in href:
                continue

            post_id = href.split("logNo=")[-1].split("&")[0]

            if post_id in seen or post_id in existing_ids:
                continue
            seen.add(post_id)

            if not href.startswith("http"):
                href = f"https://m.blog.naver.com{href}"

            new_posts.append({
                "post_id": post_id,
                "url": href
            })

            if len(new_posts) >= limit:
                break

        except Exception:
            continue

    print(f"  ìƒˆ í¬ìŠ¤íŠ¸: {len(new_posts)}ê°œ ë°œê²¬")

    # ê° ìƒˆ í¬ìŠ¤íŠ¸ ë‚´ìš© ìˆ˜ì§‘
    for post in new_posts:
        try:
            await page.goto(post["url"])
            await page.wait_for_timeout(2000)

            # ì œëª©
            post["title"] = ""
            for sel in ["h3.se_title", "[class*='title']", "h3"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["title"] = (await elem.inner_text()).strip()[:200]
                    if post["title"]:
                        break

            # ë‚ ì§œ
            post["date"] = ""
            for sel in ["[class*='date']", "time"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["date"] = (await elem.inner_text()).strip()
                    break

            # ë³¸ë¬¸
            post["content"] = ""
            for sel in ["div.se-main-container", "div#postViewArea"]:
                elem = await page.query_selector(sel)
                if elem:
                    post["content"] = (await elem.inner_text()).strip()[:10000]
                    break

            # í‹°ì»¤ ì¶”ì¶œ
            content = post.get("content", "")
            raw_tickers = set(re.findall(r'\b([A-Z]{2,5})\b', content))
            # ì¼ë°˜ ë‹¨ì–´ ì œì™¸
            common = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'SEC', 'ETF', 'CEO', 'IPO', 'FDA', 'NYSE', 'USD', 'KRW', 'SPAC', 'PIPE'}
            post["tickers"] = list(raw_tickers - common)

            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = []
            kw_map = {
                "ìˆìŠ¤í€´ì¦ˆ": ["ìˆìŠ¤í€´ì¦ˆ", "short squeeze", "ìˆì»¤ë²„"],
                "ê°•ì œì²­ì‚°": ["ê°•ì œì²­ì‚°", "forced buy", "close out"],
                "RegSHO": ["regsho", "reg sho", "threshold"],
                "FTD": ["ftd", "fail to deliver"],
                "ê¸‰ë“±": ["ê¸‰ë“±", "í­ë“±", "ìƒìŠ¹"],
                "ê¸‰ë½": ["ê¸‰ë½", "í­ë½", "í•˜ë½"],
            }
            content_lower = content.lower()
            for kw, patterns in kw_map.items():
                if any(p.lower() in content_lower for p in patterns):
                    keywords.append(kw)
            post["keywords"] = keywords

            print(f"    â†’ {post['title'][:40]}... | í‹°ì»¤: {post['tickers'][:5]}")

        except Exception as e:
            print(f"    â†’ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return new_posts


# ============================================================
# ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘
# ============================================================

async def collect_blogger_ticker_info(page, tickers):
    """ë¸”ë¡œê±°ê°€ ì–¸ê¸‰í•œ í‹°ì»¤ë“¤ì˜ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘"""
    ticker_info = {}

    # ë³´ìœ  ì¢…ëª© ì œì™¸, ìƒˆë¡œ ì–¸ê¸‰ëœ í‹°ì»¤ë§Œ
    my_tickers = {h["ticker"] for h in HOLDINGS}
    new_tickers = [t for t in tickers if t not in my_tickers][:5]  # ìµœëŒ€ 5ê°œ

    for ticker in new_tickers:
        try:
            url = f"https://www.benzinga.com/quote/{ticker.lower()}"
            await page.goto(url)
            await page.wait_for_timeout(2000)

            text = await page.inner_text("body")

            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            price_match = re.search(r'\$(\d+\.?\d*)', text)
            change_match = re.search(r'([+-]?\d+\.?\d*)\s*%', text)

            ticker_info[ticker] = {
                "price": float(price_match.group(1)) if price_match else None,
                "change_pct": float(change_match.group(1)) if change_match else None,
                "collected_at": datetime.now().isoformat()
            }

            print(f"    {ticker}: ${ticker_info[ticker]['price']} ({ticker_info[ticker]['change_pct']}%)")

        except Exception as e:
            print(f"    {ticker}: ìˆ˜ì§‘ ì‹¤íŒ¨")

    return ticker_info


# ============================================================
# DB ì €ì¥
# ============================================================

def save_to_db(prices, regSHO, exchange_rate, blog_posts, blogger_tickers, squeeze_data=None):
    """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    conn = get_db()
    cur = conn.cursor()

    # ì£¼ê°€ ì €ì¥
    for ticker, data in prices.items():
        cur.execute("""
            INSERT INTO stock_prices (ticker, regular_price, afterhours_price, premarket_price, change_percent, source)
            VALUES (%s, %s, %s, %s, %s, 'benzinga')
        """, (ticker, data.get("regular"), data.get("afterhours"), data.get("premarket"), data.get("change_pct")))

    # RegSHO ì €ì¥ (ì—°ì† ë“±ì¬ì¼ ì¶”ì )
    # ì–´ì œ ë“±ì¬ëœ í‹°ì»¤ì™€ first_seen_date ê°€ì ¸ì˜¤ê¸°
    cur.execute("""
        SELECT ticker, first_seen_date FROM regsho_list
        WHERE collected_date = (SELECT MAX(collected_date) FROM regsho_list WHERE collected_date < CURRENT_DATE)
    """)
    prev_tickers = {row[0]: row[1] for row in cur.fetchall()}

    # ì˜¤ëŠ˜ ë°ì´í„° ì‚­ì œ í›„ ìƒˆë¡œ ì €ì¥
    cur.execute("DELETE FROM regsho_list WHERE collected_date = CURRENT_DATE")
    for item in regSHO:
        ticker = item["ticker"]
        # ì–´ì œë„ ìˆì—ˆìœ¼ë©´ first_seen_date ìœ ì§€, ì•„ë‹ˆë©´ ì˜¤ëŠ˜ ë‚ ì§œ
        first_seen = prev_tickers.get(ticker, None)
        if first_seen:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, %s)
            """, (ticker, item["name"], item["market"], first_seen))
        else:
            cur.execute("""
                INSERT INTO regsho_list (ticker, security_name, market_category, first_seen_date)
                VALUES (%s, %s, %s, CURRENT_DATE)
            """, (ticker, item["name"], item["market"]))

    # í™˜ìœ¨ ì €ì¥
    cur.execute("""
        INSERT INTO exchange_rates (from_currency, to_currency, rate)
        VALUES ('USD', 'KRW', %s)
    """, (exchange_rate,))

    # ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì €ì¥ (ìƒˆ ê¸€ë§Œ)
    for post in blog_posts:
        cur.execute("""
            INSERT INTO blog_posts (post_id, title, content, tickers, keywords, post_date, url, is_new)
            VALUES (%s, %s, %s, %s, %s, %s, %s, TRUE)
            ON CONFLICT (post_id) DO NOTHING
        """, (
            post["post_id"],
            post.get("title"),
            post.get("content"),
            post.get("tickers", []),
            post.get("keywords", []),
            post.get("date"),
            post.get("url")
        ))

    # ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ì €ì¥
    for ticker, info in blogger_tickers.items():
        cur.execute("""
            INSERT INTO blogger_tickers (ticker, ticker_info)
            VALUES (%s, %s)
        """, (ticker, json.dumps(info)))

    # ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ì €ì¥ (v2)
    if squeeze_data:
        for ticker, data in squeeze_data.items():
            if data.get("squeeze_score") is not None:
                cur.execute("""
                    INSERT INTO squeeze_data (ticker, borrow_rate, short_interest, days_to_cover, short_volume, squeeze_score, available_shares, float_shares, dilution_protected, has_positive_news, has_negative_news, source)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, 'v2_combined')
                """, (
                    ticker,
                    data.get("borrow_rate"),
                    data.get("short_interest"),
                    data.get("days_to_cover"),
                    data.get("short_volume"),
                    data.get("squeeze_score"),
                    data.get("available_shares"),
                    data.get("float_shares"),
                    data.get("dilution_protected", False),
                    data.get("has_positive_news", False),
                    data.get("has_negative_news", False),
                ))

    # ë¸Œë¦¬í•‘ JSON ìƒì„± ë° ì €ì¥
    briefing = generate_briefing(prices, regSHO, exchange_rate, blog_posts, blogger_tickers)
    cur.execute("DELETE FROM stock_briefing WHERE briefing_date = CURRENT_DATE")
    cur.execute("""
        INSERT INTO stock_briefing (briefing_date, briefing_json)
        VALUES (CURRENT_DATE, %s)
    """, (json.dumps(briefing, ensure_ascii=False),))

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… DB ì €ì¥ ì™„ë£Œ")


def generate_briefing(prices, regSHO, exchange_rate, blog_posts, blogger_tickers):
    """ë¸Œë¦¬í•‘ JSON ìƒì„±"""
    # í¬íŠ¸í´ë¦¬ì˜¤ ê³„ì‚°
    portfolio = []
    total_value = 0
    total_cost = 0

    for holding in HOLDINGS:
        ticker = holding["ticker"]
        data = prices.get(ticker, {})

        # ìš°ì„ ìˆœìœ„: ì• í”„í„° > í”„ë¦¬ > ì¢…ê°€
        current = data.get("afterhours") or data.get("premarket") or data.get("regular") or 0

        value = holding["shares"] * current
        cost = holding["shares"] * holding["avg_cost"]
        gain = value - cost
        gain_pct = (gain / cost * 100) if cost > 0 else 0

        portfolio.append({
            "ticker": ticker,
            "shares": holding["shares"],
            "avg_cost": holding["avg_cost"],
            "regular_price": data.get("regular"),
            "afterhours_price": data.get("afterhours"),
            "premarket_price": data.get("premarket"),
            "current_price": current,
            "value": round(value, 2),
            "gain": round(gain, 2),
            "gain_pct": round(gain_pct, 1)
        })

        total_value += value
        total_cost += cost

    total_gain = total_value - total_cost
    total_gain_krw = total_gain * exchange_rate

    # ì„¸ê¸ˆ ê³„ì‚°
    taxable = max(0, total_gain_krw - 2500000)
    tax = taxable * 0.22
    net_profit = total_gain_krw - tax

    # RegSHO ì²´í¬
    regSHO_tickers = {item["ticker"] for item in regSHO}
    holdings_in_regSHO = [h["ticker"] for h in HOLDINGS if h["ticker"] in regSHO_tickers]

    # ìƒˆ ë¸”ë¡œê·¸ ê¸€ ìš”ì•½
    new_blog_posts = []
    all_blog_tickers = set()
    for post in blog_posts:
        new_blog_posts.append({
            "title": post.get("title", "")[:100],
            "url": post.get("url"),
            "tickers": post.get("tickers", [])[:10],
            "keywords": post.get("keywords", []),
            "date": post.get("date")
        })
        all_blog_tickers.update(post.get("tickers", []))

    return {
        "timestamp": datetime.now().isoformat(),
        "exchange_rate": exchange_rate,
        "portfolio": portfolio,
        "total": {
            "value_usd": round(total_value, 2),
            "value_krw": round(total_value * exchange_rate, 0),
            "gain_usd": round(total_gain, 2),
            "gain_krw": round(total_gain_krw, 0),
            "gain_pct": round((total_gain / total_cost * 100) if total_cost > 0 else 0, 1)
        },
        "tax": {
            "taxable_krw": round(taxable, 0),
            "tax_krw": round(tax, 0),
            "net_profit_krw": round(net_profit, 0)
        },
        "regSHO": {
            "total_count": len(regSHO),
            "holdings_on_list": holdings_in_regSHO,
            "top_tickers": [item["ticker"] for item in regSHO[:15]]
        },
        "new_blog_posts": new_blog_posts,
        "blogger_mentioned_tickers": list(all_blog_tickers),
        "blogger_ticker_info": blogger_tickers
    }


# ============================================================
# ë©”ì¸
# ============================================================

async def main():
    print("=" * 60)
    print(f"ğŸ“Š Stock Data Collector v2 - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)

    init_db()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # 1. ì£¼ê°€ ìˆ˜ì§‘ (í¬íŠ¸í´ë¦¬ì˜¤ + ì›Œì¹˜ë¦¬ìŠ¤íŠ¸)
        print("\nğŸ“ˆ ì£¼ê°€ ìˆ˜ì§‘...")
        tickers = [h["ticker"] for h in HOLDINGS] + [w["ticker"] for w in WATCHLIST]
        prices = await collect_stock_prices(page, tickers)

        # 2. RegSHO ìˆ˜ì§‘
        print("\nğŸ“‹ RegSHO ìˆ˜ì§‘...")
        regSHO = await collect_regSHO(page)

        # 3. í™˜ìœ¨ ìˆ˜ì§‘
        print("\nğŸ’± í™˜ìœ¨ ìˆ˜ì§‘...")
        exchange_rate = await collect_exchange_rate(page)

        # 4. ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)
        print("\nğŸ“ ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ìƒˆ ê¸€ë§Œ)...")
        blog_posts = await collect_blog_posts(page, limit=10)

        # 5. ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘
        all_mentioned = set()
        for post in blog_posts:
            all_mentioned.update(post.get("tickers", []))

        blogger_tickers = {}
        if all_mentioned:
            print("\nğŸ” ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤ ì •ë³´ ìˆ˜ì§‘...")
            blogger_tickers = await collect_blogger_ticker_info(page, list(all_mentioned))

        # 6. ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘ (í¬íŠ¸í´ë¦¬ì˜¤ + RegSHO ì¢…ëª©)
        print("\nğŸ”¥ ìˆìŠ¤í€´ì¦ˆ ë°ì´í„° ìˆ˜ì§‘...")
        squeeze_tickers = list(set(tickers + [r["ticker"] for r in regSHO]))
        squeeze_data = await collect_squeeze_data(page, squeeze_tickers)

        await browser.close()

    # DB ì €ì¥
    print("\nğŸ’¾ DB ì €ì¥...")
    save_to_db(prices, regSHO, exchange_rate, blog_posts, blogger_tickers, squeeze_data)

    # í‹°ì»¤ ì •ë³´(íšŒì‚¬ëª…) ì—…ë°ì´íŠ¸
    print("\nğŸ“› í‹°ì»¤ ì •ë³´ ì—…ë°ì´íŠ¸...")
    update_ticker_info(tickers)

    # ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"  - ì£¼ê°€: {len(prices)}ê°œ")
    print(f"  - RegSHO: {len(regSHO)}ê°œ")
    print(f"  - ìƒˆ ë¸”ë¡œê·¸ ê¸€: {len(blog_posts)}ê°œ")
    print(f"  - ë¸”ë¡œê±° ì–¸ê¸‰ í‹°ì»¤: {len(blogger_tickers)}ê°œ")
    print(f"  - ìˆìŠ¤í€´ì¦ˆ ë°ì´í„°: {len(squeeze_data)}ê°œ")
    print("=" * 60)

    # í‘¸ì‹œ ì•Œë¦¼ ë°œì†¡
    print("\nğŸ”” í‘¸ì‹œ ì•Œë¦¼ ë°œì†¡...")
    try:
        from api.notifications import send_data_update_notification
        result = send_data_update_notification()
        print(f"  - ë°œì†¡: {result.get('sent', 0)}ê±´, ë§Œë£Œ ì‚­ì œ: {result.get('expired', 0)}ê±´")
    except Exception as e:
        print(f"  - ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    asyncio.run(main())
