"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ìºë„ˆ

ë°ì´í„° ì†ŒìŠ¤:
1. SEC EDGAR - 8-K, 10-K ê³µì‹œ
2. Finviz - ë‰´ìŠ¤ í—¤ë“œë¼ì¸
3. Yahoo Finance - ë‰´ìŠ¤

ì‹¤í–‰:
    uv run python scanners/news_collector.py
    uv run python scanners/news_collector.py --test  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
"""

import os
import sys
import json
import time
import re
import argparse
from datetime import datetime, timedelta
from typing import Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import requests
from bs4 import BeautifulSoup
import feedparser

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from db import get_db
from psycopg2.extras import RealDictCursor

# SEC EDGAR RSS í”¼ë“œ
SEC_RSS_URL = "https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&type=8-K&company=&dateb=&owner=include&count=100&output=atom"

# í˜¸ì¬/ì•…ì¬ í‚¤ì›Œë“œ
POSITIVE_KEYWORDS = [
    'partnership', 'agreement', 'contract', 'deal', 'acquisition',
    'merger', 'fda approval', 'breakthrough', 'patent', 'revenue growth',
    'earnings beat', 'upgrade', 'buy rating', 'strong buy', 'outperform'
]

NEGATIVE_KEYWORDS = [
    'lawsuit', 'litigation', 'bankruptcy', 'default', 'fraud',
    'investigation', 'delisting', 'downgrade', 'sell rating', 'underperform',
    'recall', 'warning', 'loss', 'decline', 'cut'
]


def init_tables():
    """ë‰´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
    conn = get_db()
    cur = conn.cursor()

    cur.execute("""
        CREATE TABLE IF NOT EXISTS news_mentions (
            id SERIAL PRIMARY KEY,
            ticker VARCHAR(10) NOT NULL,
            source VARCHAR(50) NOT NULL,
            headline TEXT NOT NULL,
            url TEXT,
            sentiment VARCHAR(20) DEFAULT 'neutral',
            sentiment_score FLOAT DEFAULT 0,
            keywords TEXT[],
            published_at TIMESTAMP WITH TIME ZONE,
            collected_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(ticker, headline, source)
        );

        CREATE INDEX IF NOT EXISTS idx_news_ticker ON news_mentions(ticker);
        CREATE INDEX IF NOT EXISTS idx_news_collected ON news_mentions(collected_at);
        CREATE INDEX IF NOT EXISTS idx_news_sentiment ON news_mentions(sentiment);
    """)

    # ì¼ì¼ ë‰´ìŠ¤ ì ìˆ˜ ì§‘ê³„ í…Œì´ë¸”
    cur.execute("""
        CREATE TABLE IF NOT EXISTS daily_news_scores (
            id SERIAL PRIMARY KEY,
            scan_date DATE NOT NULL,
            ticker VARCHAR(10) NOT NULL,
            positive_count INTEGER DEFAULT 0,
            negative_count INTEGER DEFAULT 0,
            neutral_count INTEGER DEFAULT 0,
            total_score FLOAT DEFAULT 0,
            sources TEXT[],
            top_headlines TEXT[],
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(scan_date, ticker)
        );

        CREATE INDEX IF NOT EXISTS idx_daily_news_date ON daily_news_scores(scan_date);
        CREATE INDEX IF NOT EXISTS idx_daily_news_score ON daily_news_scores(total_score DESC);
    """)

    conn.commit()
    cur.close()
    conn.close()
    print("âœ… í…Œì´ë¸” ì´ˆê¸°í™” ì™„ë£Œ")


def extract_ticker_from_text(text: str) -> list:
    """í…ìŠ¤íŠ¸ì—ì„œ í‹°ì»¤ ì‹¬ë³¼ ì¶”ì¶œ"""
    # ì¼ë°˜ì ì¸ í‹°ì»¤ íŒ¨í„´: 1-5ê¸€ì ëŒ€ë¬¸ì
    pattern = r'\b([A-Z]{1,5})\b'
    matches = re.findall(pattern, text)

    # ì¼ë°˜ ë‹¨ì–´ ì œì™¸
    common_words = {'THE', 'AND', 'FOR', 'INC', 'LLC', 'CEO', 'CFO', 'SEC', 'FDA', 'IPO', 'NYSE', 'NASDAQ'}
    tickers = [m for m in matches if m not in common_words and len(m) >= 2]

    return list(set(tickers))


def analyze_sentiment(text: str) -> tuple:
    """
    í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ (ë£° ê¸°ë°˜)

    Returns:
        tuple: (sentiment, score, keywords)
    """
    text_lower = text.lower()

    pos_found = [kw for kw in POSITIVE_KEYWORDS if kw in text_lower]
    neg_found = [kw for kw in NEGATIVE_KEYWORDS if kw in text_lower]

    pos_score = len(pos_found) * 2
    neg_score = len(neg_found) * 3  # ì•…ì¬ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜

    total_score = pos_score - neg_score

    if total_score > 0:
        sentiment = 'positive'
    elif total_score < 0:
        sentiment = 'negative'
    else:
        sentiment = 'neutral'

    keywords = pos_found + neg_found

    return sentiment, total_score, keywords


def fetch_sec_edgar() -> list:
    """SEC EDGAR 8-K ê³µì‹œ ìˆ˜ì§‘"""
    print("ğŸ“„ SEC EDGAR ìˆ˜ì§‘ ì¤‘...")

    try:
        headers = {
            'User-Agent': 'DailyStockStory/1.0 (sean8320@gmail.com)'
        }

        feed = feedparser.parse(SEC_RSS_URL)

        results = []
        for entry in feed.entries[:50]:  # ìµœê·¼ 50ê°œ
            title = entry.get('title', '')
            link = entry.get('link', '')
            published = entry.get('published', '')

            # í‹°ì»¤ ì¶”ì¶œ
            tickers = extract_ticker_from_text(title)

            # ê°ì„± ë¶„ì„
            sentiment, score, keywords = analyze_sentiment(title)

            for ticker in tickers:
                results.append({
                    'ticker': ticker,
                    'source': 'SEC_EDGAR',
                    'headline': title[:500],
                    'url': link,
                    'sentiment': sentiment,
                    'sentiment_score': score,
                    'keywords': keywords,
                    'published_at': published
                })

        print(f"  â†’ SEC EDGAR: {len(results)}ê±´")
        return results

    except Exception as e:
        print(f"  âŒ SEC EDGAR ì˜¤ë¥˜: {e}")
        return []


def fetch_finviz_news(ticker: str) -> list:
    """Finvizì—ì„œ íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
    try:
        url = f"https://finviz.com/quote.ashx?t={ticker}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')
        news_table = soup.find('table', {'id': 'news-table'})

        if not news_table:
            return []

        results = []
        rows = news_table.find_all('tr')[:10]  # ìµœê·¼ 10ê°œ

        for row in rows:
            link = row.find('a')
            if not link:
                continue

            headline = link.text.strip()
            news_url = link.get('href', '')

            sentiment, score, keywords = analyze_sentiment(headline)

            results.append({
                'ticker': ticker,
                'source': 'Finviz',
                'headline': headline[:500],
                'url': news_url,
                'sentiment': sentiment,
                'sentiment_score': score,
                'keywords': keywords,
                'published_at': None
            })

        return results

    except Exception as e:
        return []


def fetch_yahoo_news(ticker: str) -> list:
    """Yahoo Financeì—ì„œ íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
    try:
        url = f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # Yahoo Finance ë‰´ìŠ¤ API
        news_url = f"https://query2.finance.yahoo.com/v1/finance/search?q={ticker}&newsCount=10"
        response = requests.get(news_url, headers=headers, timeout=10)

        if response.status_code != 200:
            return []

        data = response.json()
        news_items = data.get('news', [])

        results = []
        for item in news_items:
            headline = item.get('title', '')
            news_url = item.get('link', '')
            pub_time = item.get('providerPublishTime', 0)

            sentiment, score, keywords = analyze_sentiment(headline)

            published_at = None
            if pub_time:
                published_at = datetime.fromtimestamp(pub_time).isoformat()

            results.append({
                'ticker': ticker,
                'source': 'Yahoo',
                'headline': headline[:500],
                'url': news_url,
                'sentiment': sentiment,
                'sentiment_score': score,
                'keywords': keywords,
                'published_at': published_at
            })

        return results

    except Exception as e:
        return []


def get_trending_tickers() -> list:
    """íŠ¸ë Œë”© ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (RegSHO + ê¸°ì¡´ ê´€ì‹¬ì¢…ëª©)"""
    conn = get_db()
    cur = conn.cursor(cursor_factory=RealDictCursor)

    tickers = set()

    # RegSHO ì¢…ëª©
    cur.execute("""
        SELECT DISTINCT ticker FROM regsho_list
        WHERE collected_date >= CURRENT_DATE - INTERVAL '7 days'
    """)
    for row in cur.fetchall():
        tickers.add(row['ticker'])

    # ì‚¬ìš©ì ê´€ì‹¬ì¢…ëª©
    cur.execute("SELECT DISTINCT ticker FROM user_watchlist")
    for row in cur.fetchall():
        tickers.add(row['ticker'])

    # ì‚¬ìš©ì í¬íŠ¸í´ë¦¬ì˜¤
    cur.execute("SELECT DISTINCT ticker FROM user_holdings")
    for row in cur.fetchall():
        tickers.add(row['ticker'])

    cur.close()
    conn.close()

    return list(tickers)


def save_news(news_list: list):
    """ë‰´ìŠ¤ ì €ì¥"""
    if not news_list:
        return 0

    conn = get_db()
    cur = conn.cursor()

    saved = 0
    for news in news_list:
        try:
            cur.execute("""
                INSERT INTO news_mentions
                (ticker, source, headline, url, sentiment, sentiment_score, keywords, published_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (ticker, headline, source) DO NOTHING
            """, (
                news['ticker'],
                news['source'],
                news['headline'],
                news.get('url'),
                news['sentiment'],
                news['sentiment_score'],
                news.get('keywords', []),
                news.get('published_at')
            ))
            saved += cur.rowcount
        except Exception as e:
            print(f"  ì €ì¥ ì˜¤ë¥˜: {e}")

    conn.commit()
    cur.close()
    conn.close()

    return saved


def calculate_daily_scores():
    """ì¼ì¼ ë‰´ìŠ¤ ì ìˆ˜ ì§‘ê³„"""
    print("\nğŸ“Š ì¼ì¼ ì ìˆ˜ ì§‘ê³„ ì¤‘...")

    conn = get_db()
    cur = conn.cursor(cursor_factory=RealDictCursor)

    # ì˜¤ëŠ˜ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì§‘ê³„
    cur.execute("""
        SELECT
            ticker,
            COUNT(*) FILTER (WHERE sentiment = 'positive') as positive_count,
            COUNT(*) FILTER (WHERE sentiment = 'negative') as negative_count,
            COUNT(*) FILTER (WHERE sentiment = 'neutral') as neutral_count,
            SUM(sentiment_score) as total_score,
            ARRAY_AGG(DISTINCT source) as sources,
            ARRAY_AGG(headline ORDER BY sentiment_score DESC) as headlines
        FROM news_mentions
        WHERE collected_at >= CURRENT_DATE
        GROUP BY ticker
        HAVING COUNT(*) >= 1
    """)

    rows = cur.fetchall()

    for row in rows:
        # ìƒìœ„ 3ê°œ í—¤ë“œë¼ì¸ë§Œ ì €ì¥
        top_headlines = row['headlines'][:3] if row['headlines'] else []

        cur.execute("""
            INSERT INTO daily_news_scores
            (scan_date, ticker, positive_count, negative_count, neutral_count,
             total_score, sources, top_headlines)
            VALUES (CURRENT_DATE, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (scan_date, ticker)
            DO UPDATE SET
                positive_count = EXCLUDED.positive_count,
                negative_count = EXCLUDED.negative_count,
                neutral_count = EXCLUDED.neutral_count,
                total_score = EXCLUDED.total_score,
                sources = EXCLUDED.sources,
                top_headlines = EXCLUDED.top_headlines
        """, (
            row['ticker'],
            row['positive_count'],
            row['negative_count'],
            row['neutral_count'],
            row['total_score'] or 0,
            row['sources'],
            top_headlines
        ))

    conn.commit()
    cur.close()
    conn.close()

    print(f"  â†’ {len(rows)}ê°œ ì¢…ëª© ì ìˆ˜ ì§‘ê³„ ì™„ë£Œ")


def get_top_buzz(limit: int = 50) -> list:
    """ë‰´ìŠ¤ ì ìˆ˜ ìƒìœ„ ì¢…ëª© ì¡°íšŒ"""
    conn = get_db()
    cur = conn.cursor(cursor_factory=RealDictCursor)

    cur.execute("""
        SELECT ticker, positive_count, negative_count, total_score, sources, top_headlines
        FROM daily_news_scores
        WHERE scan_date = CURRENT_DATE
          AND total_score > 0  -- ê¸ì • > ë¶€ì •
        ORDER BY total_score DESC
        LIMIT %s
    """, (limit,))

    results = cur.fetchall()
    cur.close()
    conn.close()

    return results


def is_us_market_holiday() -> bool:
    """ë¯¸êµ­ ì¦ì‹œ íœ´ì¥ì¼ ì²´í¬"""
    from datetime import date
    today = date.today()

    # ì£¼ë§
    if today.weekday() >= 5:
        return True

    # 2026ë…„ ë¯¸êµ­ ì¦ì‹œ íœ´ì¥ì¼
    holidays_2026 = [
        date(2026, 1, 1),   # ìƒˆí•´
        date(2026, 1, 19),  # MLK Day
        date(2026, 2, 16),  # Presidents Day
        date(2026, 4, 3),   # Good Friday
        date(2026, 5, 25),  # Memorial Day
        date(2026, 7, 3),   # Independence Day (observed)
        date(2026, 9, 7),   # Labor Day
        date(2026, 11, 26), # Thanksgiving
        date(2026, 12, 25), # Christmas
    ]

    return today in holidays_2026


def main():
    parser = argparse.ArgumentParser(description='ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ìºë„ˆ')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ')
    parser.add_argument('--force', action='store_true', help='íœ´ì¥ì¼ ë¬´ì‹œ')
    args = parser.parse_args()

    print("=" * 50)
    print("ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ìºë„ˆ ì‹œì‘")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 50)

    # íœ´ì¥ì¼ ì²´í¬
    if is_us_market_holiday() and not args.force:
        print("ğŸ“… ë¯¸êµ­ ì¦ì‹œ íœ´ì¥ì¼ - ìˆ˜ì§‘ ê±´ë„ˆëœ€")
        return

    # í…Œì´ë¸” ì´ˆê¸°í™”
    init_tables()

    all_news = []

    # 1. SEC EDGAR ìˆ˜ì§‘
    sec_news = fetch_sec_edgar()
    all_news.extend(sec_news)

    # 2. íŠ¸ë Œë”© ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
    tickers = get_trending_tickers()
    print(f"\nğŸ” {len(tickers)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    if args.test:
        tickers = tickers[:5]  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œë§Œ
        print("  (í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 5ê°œ ì¢…ëª©ë§Œ)")

    # ë³‘ë ¬ ìˆ˜ì§‘
    with ThreadPoolExecutor(max_workers=5) as executor:
        finviz_futures = {executor.submit(fetch_finviz_news, t): t for t in tickers}
        yahoo_futures = {executor.submit(fetch_yahoo_news, t): t for t in tickers}

        for future in as_completed(finviz_futures):
            news = future.result()
            all_news.extend(news)

        for future in as_completed(yahoo_futures):
            news = future.result()
            all_news.extend(news)

        # Rate limit
        time.sleep(0.5)

    print(f"\nğŸ’¾ ì´ {len(all_news)}ê±´ ìˆ˜ì§‘ë¨")

    # 3. ì €ì¥
    saved = save_news(all_news)
    print(f"  â†’ {saved}ê±´ ì‹ ê·œ ì €ì¥")

    # 4. ì¼ì¼ ì ìˆ˜ ì§‘ê³„
    calculate_daily_scores()

    # 5. ë‰´ìŠ¤ ë²¡í„°í™” + ì¤‘ë³µ ê°ì§€ + ì‹œê°„ ê°€ì¤‘ì¹˜
    try:
        from lib.news_vectors import embed_and_dedup as _embed_dedup, init_vector_tables as _init_vt, calculate_time_weights as _calc_tw
        _init_vt()
        _embed_dedup()
        _calc_tw()
    except Exception as e:
        print(f"  âš ï¸ ë‰´ìŠ¤ ë²¡í„°í™” ìŠ¤í‚µ (ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì •ìƒ): {e}")

    # 6. ìƒìœ„ ì¢…ëª© ì¶œë ¥
    top = get_top_buzz(10)
    if top:
        print("\nğŸ”¥ ë‰´ìŠ¤ ì ìˆ˜ TOP 10:")
        for i, item in enumerate(top, 1):
            sentiment_emoji = "ğŸ“ˆ" if item['total_score'] > 5 else "ğŸ“Š"
            print(f"  {i}. {item['ticker']}: {sentiment_emoji} ì ìˆ˜={item['total_score']:.1f} "
                  f"(+{item['positive_count']}/-{item['negative_count']})")

    print("\nâœ… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")


if __name__ == "__main__":
    main()
